{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# CNN for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "#os.chdir(\"D:\\semicolon\\Deep Learning\");\n",
    "df=pd.read_csv('jokes.csv');\n",
    "\n",
    "\n",
    "x=df['Question'].values.tolist()\n",
    "y=df['Answer'].values.tolist()\n",
    "\n",
    "corpus= x+y\n",
    "  \n",
    "tok_corp= [nltk.word_tokenize(sent.decode('utf-8')) for sent in corpus\n",
    "           \n",
    "model = gensim.models.Word2Vec(tok_corp, min_count=1, size = 32)\n",
    "\n",
    "#model.save('testmodel')\n",
    "#model = gensim.models.Word2Vec.load('test_model')\n",
    "#model.most_similar('word')\n",
    "#model.most_similar([vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Trilingual', 0.6726022958755493),\n",
       " (u'maled', 0.667369544506073),\n",
       " (u'chicken', 0.6664585471153259),\n",
       " (u'turducken', 0.641279399394989),\n",
       " (u'plum', 0.6358986496925354),\n",
       " (u'Refrigerators', 0.6274476051330566),\n",
       " (u'cow', 0.6211307048797607),\n",
       " (u'Eye-deer', 0.6191414594650269),\n",
       " (u'Painter', 0.6179731488227844),\n",
       " (u'exterminator', 0.6149735450744629)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PARAMS: {'q': 'Obama'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@REDACT_PSUDONYM @MargaretsBelly @realDonaldTrump Lol no different than what Obama faced.\n",
      "Sentiment(polarity=0.4, subjectivity=0.6499999999999999)\n",
      "\n",
      "RT @robertsjonathan: Everyone goes mental at Trump when he threatens to retaliate against North Korea. None of them minded Obama doing the…\n",
      "Sentiment(polarity=-0.1, subjectivity=0.2)\n",
      "\n",
      "RT @RexTilllerson: #UNGA\n",
      "FOR 8 YEARS: Obama repeatedly apologized to the World for America's Greatness\n",
      "\n",
      "TODAY: Trump boasts to the World of…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "RT @SethAMandel: Watching Obama/Clinton hands and the press defend Venezuela and North Korea because they hate Trump is depressing.\n",
      "Sentiment(polarity=-0.7, subjectivity=0.9)\n",
      "\n",
      "RT @horowitz39: Awaiting apologies from media, \"liberals.\" NeverTrumpers who called  Trump a liar for saying his team was wiretapped https:…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "@ashleymarin16 @15_RNG @thetrueed @funder @altjellyroller If Obama would of done something so stupid, they would be… https://t.co/6bTz17qy4X\n",
      "Sentiment(polarity=-0.7999999999999999, subjectivity=1.0)\n",
      "\n",
      "RT @DefendTheWeb: Trump is VINDICATED! Obama Feds Wiretapped Campaign! #RT #Trump45 #ObamaLegacy #tcot #P2 #MAGA https://t.co/hsmp2CKbMz\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "RT @LauraLoomer: God forbid a president actually calls out corrupt/ barbaric Muslim Nations, terror, &amp; North Korea's insane dictator! I kno…\n",
      "Sentiment(polarity=-0.5, subjectivity=0.55)\n",
      "\n",
      "RT @RealJamesWoods: Trump Vindicated: Shock Report Says Obama Government Wiretapped Trump Campaign #WinningEveryDay https://t.co/HMOJkicY5y\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "RT @Impeach_D_Trump: For the FINAL F*cking Time\n",
      "\n",
      "A Legal FISA Warrant Against Paul Manafort, does NOT Vindicate Trump's Claim that Obama IL…\n",
      "Sentiment(polarity=-0.13333333333333333, subjectivity=0.6666666666666666)\n",
      "\n",
      "RT @RexTilllerson: Paul Manafort's house was raided by the FBI after he was illegally wire tapped by Barack Hussein Obama https://t.co/zd85…\n",
      "Sentiment(polarity=-0.5, subjectivity=0.5)\n",
      "\n",
      "🇺🇸has now become the embarrassment that @FoxNews @foxandfriends @seanhannity @LouDobbs @greggutfeld @kilmeade 🗣 of… https://t.co/w9vyFr5Lbq\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "@KDPenguins @chickermcd @Tim_McDonnell @InvestigateRU @perlmutations Obama bombed 7 countries and HRC wanted a no f… https://t.co/AVHmVnk5r8\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "RT @TrumpSuperPAC: Trump Vindicated! Comey, Clapper, Lynch, Rice &amp; Obama all LIED to Congress &amp; American People! WIRETAPS are SEDITION! htt…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "RT @DineshDSouza: So Obama WAS wiretapping @realDonaldTrump after all--once again Trump is right &amp; the media hyenas prove to be wrong\n",
      "Sentiment(polarity=-0.10714285714285715, subjectivity=0.7178571428571429)\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Average Sentiment Score: -0.16\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "## Twitter sentiment using textblob  \n",
    "import tweepy\n",
    "import numpy as np \n",
    "from textblob import TextBlob\n",
    "\n",
    "# Step 1 - Authenticate \n",
    "consumer_key= 'W7HwSRUwBJaYzuAl19JdB5Pfu'\n",
    "consumer_secret= '1cOMrX76J3rE1wWH6Q2Aw7oSk5Fogppo6B7d9HzJXwP3Yj67Iw'\n",
    "\n",
    "access_token='1098995911-MEWZz80LoXe3Kl6yDvJ6NRrc1HaEk9P68KlGTbb'\n",
    "access_token_secret='fca6OZBBpC9jqqgkWKIFW12FehQAKsho3xVpoqRYeQsbL'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Step 3 - Retrieve Tweets containing key word\n",
    "public_tweets = api.search('Obama')\n",
    "\n",
    "sentiment_polarities=[] \n",
    "\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)    \n",
    "    #Step 4 Perform Sentiment Analysis on Tweets\n",
    "    blob = TextBlob(tweet.text)\n",
    "    print(blob.sentiment)\n",
    "    #print(blob.translate(to=\"es\"))\n",
    "    sentiment_polarities.append(blob.sentiment.polarity)\n",
    "    print(\"\")\n",
    "    \n",
    "print \"------------------------------------------------------------------\"\n",
    "print \"Average Sentiment Score: %0.2f\" %np.array(sentiment_polarities).mean()  \n",
    "print len(sentiment_polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [('I love this sandwich.', 'pos'),\n",
    "('this is an amazing place!', 'pos'),\n",
    "('I feel very good about these beers.', 'pos'),\n",
    "('this is my best work.', 'pos'),\n",
    "(\"what an awesome view\", 'pos'),\n",
    "('I do not like this restaurant', 'neg'),\n",
    "('I am tired of this stuff.', 'neg'),\n",
    "(\"I can't deal with this\", 'neg'),\n",
    "('he is my sworn enemy!', 'neg'),\n",
    "('my boss is horrible.', 'neg')\n",
    "]\n",
    "\n",
    "\n",
    "test = [ ('the beer was good.', 'pos'),\n",
    "('I do not enjoy my job', 'neg'),\n",
    "(\"I ain't feeling dandy today.\", 'neg'),\n",
    "(\"I feel amazing!\", 'pos'),\n",
    "('Gary is a friend of mine.', 'pos'),\n",
    "(\"I can't believe I'm doing this.\", 'neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###What if you want to customaize your classifier based on sentences and labels:) \n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, True]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=[]\n",
    "for i in test:\n",
    "    blob=TextBlob(i[0], classifier=cl)\n",
    "    result.append(blob.classify()==i[1])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"The beer is good. But the hangover is horrible.\", classifier=cl).classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jetlag is genuinely worse than a hangover...my mind is on less than autopilot 😵😵😵😵 #jetlag #Icannot\n",
      "Sentiment(polarity=-0.2833333333333333, subjectivity=0.3333333333333333)\n",
      "\n",
      "the plan was to drink until the pain over, but what's worse the pain or the hangover?\n",
      "Sentiment(polarity=-0.4, subjectivity=0.6)\n",
      "\n",
      "@andydumamba  g na? Hahaha o hangover kpa?\n",
      "Sentiment(polarity=0.2, subjectivity=0.4)\n",
      "\n",
      "RT @Polls_king1: Funniest movie of all time round 3 part 1 #GroundhogDay  @Home_Alone_Kev   @AirplaneMovie  #Hangover\n",
      "Sentiment(polarity=-0.2, subjectivity=0.4)\n",
      "\n",
      "RT @UFCKian: Long day in work ahead. Think I'll just hide in the jacuzzi for the day #gymlife #hangover\n",
      "Sentiment(polarity=-0.05, subjectivity=0.4)\n",
      "\n",
      "RT @bilad1987: Hangover horn #cock #dick #horny #gay #bi #cockout #wank #precum #cum #sex #porn #GayDaddy #men https://t.co/SDS5nyo3Mf\n",
      "Sentiment(polarity=0.4166666666666667, subjectivity=0.5833333333333334)\n",
      "\n",
      "I think it's the hangover from the boat still\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "I been laughing by myself watching the hangover 💀💀\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "the hangover is funny as shit 😂😂😂😂😂😂😂\n",
      "Sentiment(polarity=0.024999999999999994, subjectivity=0.9)\n",
      "\n",
      "The nigga in hangover got 6 stitches and it cost 6 dollars I'm crine 😭\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "Jwu. Tamang hangover lang\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "Hangover part kesekian kali 🤣#hangover #freckles https://t.co/XFccl4foG2\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "RT @peopleofYYC: If you are hungover this weekend, Check out the YYC Hangover Pizza from @AtlasPizza1. It will fix what ails ya! #Atlaspizz…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "B word strikes again, right I'm gonna have a serious hangover #RAW\n",
      "Sentiment(polarity=-0.0927960927960928, subjectivity=0.5546398046398046)\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Average Sentiment Score: -0.03\n"
     ]
    }
   ],
   "source": [
    "# Let's use this custom classifier \n",
    "public_tweets = api.search('hangover')\n",
    "\n",
    "sentiment_polarities=[] \n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)    \n",
    "    #Step 4 Perform Sentiment Analysis on Tweets\n",
    "    blob = TextBlob(tweet.text, classifier=cl) ## custom classifier\n",
    "    print(blob.sentiment)\n",
    "    sentiment_polarities.append(blob.sentiment.polarity)\n",
    "    print(\"\")\n",
    "    #@can we do emoji sentiment? \n",
    "    \n",
    "print \"------------------------------------------------------------------\"\n",
    "print \"Average Sentiment Score: %0.2f\" %np.array(sentiment_polarities).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7039  | total loss: \u001b[1m\u001b[32m0.03504\u001b[0m\u001b[0m | time: 152.310s\n",
      "| Adam | epoch: 010 | loss: 0.03504 - acc: 0.9913 -- iter: 22496/22500\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.03304\u001b[0m\u001b[0m | time: 155.318s\n",
      "| Adam | epoch: 010 | loss: 0.03304 - acc: 0.9922 | val_loss: 0.86013 - val_acc: 0.8144 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "## Another Sentiment Analysis \n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "# IMDB Dataset loading\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n",
    "                                valid_portion=0.1)\n",
    "\n",
    "trainX, trainY = train\n",
    "testX, testY = test\n",
    "\n",
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "trainX = pad_sequences(trainX, maxlen=100, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=100, value=0.)\n",
    "\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "# Network building\n",
    "net = tflearn.input_data([None, 100])\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GloVe\n",
    "##https://github.com/stanfordnlp/GloVe\n",
    "#!cd glove && make\n",
    "#!./demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with Quora Paired Question Data: Semantic Matching  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table('quora_duplicate_questions.tsv')\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in share market in india?</td>\n",
       "      <td>What is the step by step guide to invest in share market?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?</td>\n",
       "      <td>What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet connection while using a VPN?</td>\n",
       "      <td>How can Internet speed be increased by hacking through DNS?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve it?</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] is divided by 24,23?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone and video games?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Motorolla DCX3400?</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>Method to find separation of slits using fresnel biprism?</td>\n",
       "      <td>What are some of the things technicians can tell about the durability and reliability of Laptops and its components?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>What are the laws to change your status from a student visa to a green card in the US, how do they compare to the immigration laws in Canada?</td>\n",
       "      <td>What are the laws to change your status from a student visa to a green card in the US? How do they compare to the immigration laws in Japan?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>What would a Trump presidency mean for current international master’s students on an F1 visa?</td>\n",
       "      <td>How will a Trump presidency affect the students presently in US or planning to study in US?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>Why do girls want to be friends with the guy they reject?</td>\n",
       "      <td>How do guys feel after rejecting a girl?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>Why are so many Quora users posting questions that are readily answered on Google?</td>\n",
       "      <td>Why do people ask Quora questions which can be answered easily by Google?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>Which is the best digital marketing institution in banglore?</td>\n",
       "      <td>Which is the best digital marketing institute in Pune?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>What's causing someone to be jealous?</td>\n",
       "      <td>What can I do to avoid being jealous of someone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>What are the questions should not ask on Quora?</td>\n",
       "      <td>Which question should I ask on Quora?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>How much is 30 kV in HP?</td>\n",
       "      <td>Where can I find a conversion chart for CC to horsepower?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>What does it mean that every time I look at the clock the numbers are the same?</td>\n",
       "      <td>How many times a day do a clock’s hands overlap?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>What are some tips on making it through the job interview process at Medicines?</td>\n",
       "      <td>What are some tips on making it through the job interview process at Foundation Medicine?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>What is web application?</td>\n",
       "      <td>What is the web application framework?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>Does society place too much importance on sports?</td>\n",
       "      <td>How do sports contribute to the society?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>What is best way to make money online?</td>\n",
       "      <td>What is best way to ask for money online?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>How should I prepare for CA final law?</td>\n",
       "      <td>How one should know that he/she completely prepare for CA final exam?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>141</td>\n",
       "      <td>142</td>\n",
       "      <td>What are the types of immunity?</td>\n",
       "      <td>What are the different types of immunity in our body?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>143</td>\n",
       "      <td>144</td>\n",
       "      <td>What is a narcissistic personality disorder?</td>\n",
       "      <td>What is narcissistic personality disorder?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>145</td>\n",
       "      <td>146</td>\n",
       "      <td>How I can speak English fluently?</td>\n",
       "      <td>How can I learn to speak English fluently?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>147</td>\n",
       "      <td>148</td>\n",
       "      <td>How helpful is QuickBooks' auto data recovery support phone number to recover your corrupted data files?</td>\n",
       "      <td>What is the quickbooks customer support phone number USA?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>Who is the richest gambler of all time and how can I reach his level?</td>\n",
       "      <td>Who is the richest gambler of all time and how can I reach his level as a gambler?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>151</td>\n",
       "      <td>152</td>\n",
       "      <td>If I fire a bullet backward from an aircraft going faster than the bullet; will the bullet be going backwards?</td>\n",
       "      <td>Do bullets travel faster than the speed of sound when shot from a gun? If not, is it possible? If they do, what gun and how much devastation occurs?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>How do I prevent breast cancer?</td>\n",
       "      <td>Is breast cancer preventable?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>155</td>\n",
       "      <td>156</td>\n",
       "      <td>How do I log out of my Gmail account on my friend's phone?</td>\n",
       "      <td>How can I know who logged in to my Gmail account? (by telling his IP address or device name)?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>157</td>\n",
       "      <td>158</td>\n",
       "      <td>How can I make money through the Internet?</td>\n",
       "      <td>What are some different ways to make money online, excluding selling things?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>159</td>\n",
       "      <td>160</td>\n",
       "      <td>What is purpose of life?</td>\n",
       "      <td>What's the purpose of life? What is life actually about?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>161</td>\n",
       "      <td>162</td>\n",
       "      <td>When will the BJP government strip all the Muslims and the Christians of the Indian citizenship and put them on boats like the Rohingya's of Burma?</td>\n",
       "      <td>Why India does not apply the \"Burma-Rohingya model\" to deport illegal Bangladeshis?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>What is the right etiquette for wishing a Jehovah Witness happy birthday?</td>\n",
       "      <td>How important is it to be the first person to wish someone a happy birthday?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>165</td>\n",
       "      <td>166</td>\n",
       "      <td>If someone wants to open a commercial FM radio station in any city of India, how much does it cost and what is the procedure?</td>\n",
       "      <td>I want to make a travel commercial/clip video HD , For India and New Zealand. How much will it cost?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>167</td>\n",
       "      <td>168</td>\n",
       "      <td>Why do Swiss despise Asians?</td>\n",
       "      <td>Why do technical employees despise sales people so much?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>169</td>\n",
       "      <td>170</td>\n",
       "      <td>What are some of the high salary income jobs in the field of biotechnology?</td>\n",
       "      <td>What are some high paying jobs for a fresher with an M.Tech in biotechnology?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>171</td>\n",
       "      <td>172</td>\n",
       "      <td>How can I increase my height after 21 also?</td>\n",
       "      <td>Can height increase after 25?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>173</td>\n",
       "      <td>174</td>\n",
       "      <td>What were the major effects of the cambodia earthquake, and how do these effects compare to the Kamchatca earthquakes in 1952?</td>\n",
       "      <td>What were the major effects of the cambodia earthquake, and how do these effects compare to the Valparaiso earthquake in 1822?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>175</td>\n",
       "      <td>176</td>\n",
       "      <td>What is the difference between sincerity and fairness?</td>\n",
       "      <td>What's the difference between honest and sincere?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>Which is the best gaming laptop under 60k INR?</td>\n",
       "      <td>Which is the best gaming laptop under Rs 60000?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>What is your review of The Next Warrior: Proving Grounds - Part 9?</td>\n",
       "      <td>What is your review of The Next Warrior: Proving Grounds - Part 5?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>What is the best reference book for physics class 11th?</td>\n",
       "      <td>Which book should I choose for reference for physics and chemistry (class 11, CBSE board)?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>183</td>\n",
       "      <td>184</td>\n",
       "      <td>National Institute of Technology, Kurukshetra: How is the social life at NITK, Surathkal?</td>\n",
       "      <td>National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>185</td>\n",
       "      <td>186</td>\n",
       "      <td>What are some of the best romantic movies in English?</td>\n",
       "      <td>What is the best romantic movie you have ever seen?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>What causes a nightmare?</td>\n",
       "      <td>What causes nightmares that seem real?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>What is abstract expressionism in painting?</td>\n",
       "      <td>What are some of the major influences of abstract expressionism?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "      <td>How does 3D printing work?</td>\n",
       "      <td>How do 3D printing work?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>193</td>\n",
       "      <td>194</td>\n",
       "      <td>What was it like to attend Caltech with Jeremy Ehrhardt?</td>\n",
       "      <td>Who are some notable folks who attended Caltech?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>Why did harry become a horcrux?</td>\n",
       "      <td>What is a Horcrux?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>What are the best associate product manager (APM) programs that someone in their early 20s can join to learn product management and have a rewarding career in the company?</td>\n",
       "      <td>What are the general requirement to become a Product Manager or a Program Manager in a product based software company?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>Why is the number for Skype at 1-855-425-3768 always busy?</td>\n",
       "      <td>How could I get Skype to work on an android 4.1.1 phone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2  \\\n",
       "0    0     1     2   \n",
       "1    1     3     4   \n",
       "2    2     5     6   \n",
       "3    3     7     8   \n",
       "4    4     9    10   \n",
       "5    5    11    12   \n",
       "6    6    13    14   \n",
       "7    7    15    16   \n",
       "8    8    17    18   \n",
       "9    9    19    20   \n",
       "10  10    21    22   \n",
       "11  11    23    24   \n",
       "12  12    25    26   \n",
       "13  13    27    28   \n",
       "14  14    29    30   \n",
       "15  15    31    32   \n",
       "16  16    33    34   \n",
       "17  17    35    36   \n",
       "18  18    37    38   \n",
       "19  19    39    40   \n",
       "20  20    41    42   \n",
       "21  21    43    44   \n",
       "22  22    45    46   \n",
       "23  23    47    48   \n",
       "24  24    49    50   \n",
       "25  25    51    52   \n",
       "26  26    53    54   \n",
       "27  27    55    56   \n",
       "28  28    57    58   \n",
       "29  29    59    60   \n",
       "..  ..   ...   ...   \n",
       "70  70   141   142   \n",
       "71  71   143   144   \n",
       "72  72   145   146   \n",
       "73  73   147   148   \n",
       "74  74   149   150   \n",
       "75  75   151   152   \n",
       "76  76   153   154   \n",
       "77  77   155   156   \n",
       "78  78   157   158   \n",
       "79  79   159   160   \n",
       "80  80   161   162   \n",
       "81  81   163   164   \n",
       "82  82   165   166   \n",
       "83  83   167   168   \n",
       "84  84   169   170   \n",
       "85  85   171   172   \n",
       "86  86   173   174   \n",
       "87  87   175   176   \n",
       "88  88   177   178   \n",
       "89  89   179   180   \n",
       "90  90   181   182   \n",
       "91  91   183   184   \n",
       "92  92   185   186   \n",
       "93  93   187   188   \n",
       "94  94   189   190   \n",
       "95  95   191   192   \n",
       "96  96   193   194   \n",
       "97  97   195   196   \n",
       "98  98   197   198   \n",
       "99  99   199   200   \n",
       "\n",
       "                                                                                                                                                                      question1  \\\n",
       "0                                                                                                            What is the step by step guide to invest in share market in india?   \n",
       "1                                                                                                                           What is the story of Kohinoor (Koh-i-Noor) Diamond?   \n",
       "2                                                                                                     How can I increase the speed of my internet connection while using a VPN?   \n",
       "3                                                                                                                            Why am I mentally very lonely? How can I solve it?   \n",
       "4                                                                                                  Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?   \n",
       "5                                                                                        Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?   \n",
       "6                                                                                                                                                           Should I buy tiago?   \n",
       "7                                                                                                                                                How can I be a good geologist?   \n",
       "8                                                                                                                                               When do you use シ instead of し?   \n",
       "9                                                                                                                  Motorola (company): Can I hack my Charter Motorolla DCX3400?   \n",
       "10                                                                                                                    Method to find separation of slits using fresnel biprism?   \n",
       "11                                                                                                                                  How do I read and find my YouTube comments?   \n",
       "12                                                                                                                                         What can make Physics easy to learn?   \n",
       "13                                                                                                                                  What was your first sexual experience like?   \n",
       "14                                What are the laws to change your status from a student visa to a green card in the US, how do they compare to the immigration laws in Canada?   \n",
       "15                                                                                What would a Trump presidency mean for current international master’s students on an F1 visa?   \n",
       "16                                                                                                                                                 What does manipulation mean?   \n",
       "17                                                                                                                    Why do girls want to be friends with the guy they reject?   \n",
       "18                                                                                           Why are so many Quora users posting questions that are readily answered on Google?   \n",
       "19                                                                                                                 Which is the best digital marketing institution in banglore?   \n",
       "20                                                                                                                                                   Why do rockets look white?   \n",
       "21                                                                                                                                        What's causing someone to be jealous?   \n",
       "22                                                                                                                              What are the questions should not ask on Quora?   \n",
       "23                                                                                                                                                     How much is 30 kV in HP?   \n",
       "24                                                                                              What does it mean that every time I look at the clock the numbers are the same?   \n",
       "25                                                                                              What are some tips on making it through the job interview process at Medicines?   \n",
       "26                                                                                                                                                     What is web application?   \n",
       "27                                                                                                                            Does society place too much importance on sports?   \n",
       "28                                                                                                                                       What is best way to make money online?   \n",
       "29                                                                                                                                       How should I prepare for CA final law?   \n",
       "..                                                                                                                                                                          ...   \n",
       "70                                                                                                                                              What are the types of immunity?   \n",
       "71                                                                                                                                 What is a narcissistic personality disorder?   \n",
       "72                                                                                                                                            How I can speak English fluently?   \n",
       "73                                                                     How helpful is QuickBooks' auto data recovery support phone number to recover your corrupted data files?   \n",
       "74                                                                                                        Who is the richest gambler of all time and how can I reach his level?   \n",
       "75                                                               If I fire a bullet backward from an aircraft going faster than the bullet; will the bullet be going backwards?   \n",
       "76                                                                                                                                              How do I prevent breast cancer?   \n",
       "77                                                                                                                   How do I log out of my Gmail account on my friend's phone?   \n",
       "78                                                                                                                                   How can I make money through the Internet?   \n",
       "79                                                                                                                                                     What is purpose of life?   \n",
       "80                          When will the BJP government strip all the Muslims and the Christians of the Indian citizenship and put them on boats like the Rohingya's of Burma?   \n",
       "81                                                                                                    What is the right etiquette for wishing a Jehovah Witness happy birthday?   \n",
       "82                                                If someone wants to open a commercial FM radio station in any city of India, how much does it cost and what is the procedure?   \n",
       "83                                                                                                                                                 Why do Swiss despise Asians?   \n",
       "84                                                                                                  What are some of the high salary income jobs in the field of biotechnology?   \n",
       "85                                                                                                                                  How can I increase my height after 21 also?   \n",
       "86                                               What were the major effects of the cambodia earthquake, and how do these effects compare to the Kamchatca earthquakes in 1952?   \n",
       "87                                                                                                                       What is the difference between sincerity and fairness?   \n",
       "88                                                                                                                               Which is the best gaming laptop under 60k INR?   \n",
       "89                                                                                                           What is your review of The Next Warrior: Proving Grounds - Part 9?   \n",
       "90                                                                                                                      What is the best reference book for physics class 11th?   \n",
       "91                                                                                    National Institute of Technology, Kurukshetra: How is the social life at NITK, Surathkal?   \n",
       "92                                                                                                                        What are some of the best romantic movies in English?   \n",
       "93                                                                                                                                                     What causes a nightmare?   \n",
       "94                                                                                                                                  What is abstract expressionism in painting?   \n",
       "95                                                                                                                                                   How does 3D printing work?   \n",
       "96                                                                                                                     What was it like to attend Caltech with Jeremy Ehrhardt?   \n",
       "97                                                                                                                                              Why did harry become a horcrux?   \n",
       "98  What are the best associate product manager (APM) programs that someone in their early 20s can join to learn product management and have a rewarding career in the company?   \n",
       "99                                                                                                                   Why is the number for Skype at 1-855-425-3768 always busy?   \n",
       "\n",
       "                                                                                                                                                        question2  \\\n",
       "0                                                                                                       What is the step by step guide to invest in share market?   \n",
       "1                                                                        What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?   \n",
       "2                                                                                                     How can Internet speed be increased by hacking through DNS?   \n",
       "3                                                                                               Find the remainder when [math]23^{24}[/math] is divided by 24,23?   \n",
       "4                                                                                                                         Which fish would survive in salt water?   \n",
       "5                                                                      I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?   \n",
       "6                                                                                                  What keeps childern active and far from phone and video games?   \n",
       "7                                                                                                                       What should I do to be a great geologist?   \n",
       "8                                                                                                                           When do you use \"&\" instead of \"and\"?   \n",
       "9                                                                                                               How do I hack Motorola DCX3400 for free internet?   \n",
       "10                                           What are some of the things technicians can tell about the durability and reliability of Laptops and its components?   \n",
       "11                                                                                                                         How can I see all my Youtube comments?   \n",
       "12                                                                                                                        How can you make physics easy to learn?   \n",
       "13                                                                                                                         What was your first sexual experience?   \n",
       "14                   What are the laws to change your status from a student visa to a green card in the US? How do they compare to the immigration laws in Japan?   \n",
       "15                                                                    How will a Trump presidency affect the students presently in US or planning to study in US?   \n",
       "16                                                                                                                                  What does manipulation means?   \n",
       "17                                                                                                                       How do guys feel after rejecting a girl?   \n",
       "18                                                                                      Why do people ask Quora questions which can be answered easily by Google?   \n",
       "19                                                                                                         Which is the best digital marketing institute in Pune?   \n",
       "20                                                                                                                    Why are rockets and boosters painted white?   \n",
       "21                                                                                                               What can I do to avoid being jealous of someone?   \n",
       "22                                                                                                                          Which question should I ask on Quora?   \n",
       "23                                                                                                      Where can I find a conversion chart for CC to horsepower?   \n",
       "24                                                                                                               How many times a day do a clock’s hands overlap?   \n",
       "25                                                                      What are some tips on making it through the job interview process at Foundation Medicine?   \n",
       "26                                                                                                                         What is the web application framework?   \n",
       "27                                                                                                                       How do sports contribute to the society?   \n",
       "28                                                                                                                      What is best way to ask for money online?   \n",
       "29                                                                                          How one should know that he/she completely prepare for CA final exam?   \n",
       "..                                                                                                                                                            ...   \n",
       "70                                                                                                          What are the different types of immunity in our body?   \n",
       "71                                                                                                                     What is narcissistic personality disorder?   \n",
       "72                                                                                                                     How can I learn to speak English fluently?   \n",
       "73                                                                                                      What is the quickbooks customer support phone number USA?   \n",
       "74                                                                             Who is the richest gambler of all time and how can I reach his level as a gambler?   \n",
       "75           Do bullets travel faster than the speed of sound when shot from a gun? If not, is it possible? If they do, what gun and how much devastation occurs?   \n",
       "76                                                                                                                                  Is breast cancer preventable?   \n",
       "77                                                                  How can I know who logged in to my Gmail account? (by telling his IP address or device name)?   \n",
       "78                                                                                   What are some different ways to make money online, excluding selling things?   \n",
       "79                                                                                                       What's the purpose of life? What is life actually about?   \n",
       "80                                                                            Why India does not apply the \"Burma-Rohingya model\" to deport illegal Bangladeshis?   \n",
       "81                                                                                   How important is it to be the first person to wish someone a happy birthday?   \n",
       "82                                                           I want to make a travel commercial/clip video HD , For India and New Zealand. How much will it cost?   \n",
       "83                                                                                                       Why do technical employees despise sales people so much?   \n",
       "84                                                                                  What are some high paying jobs for a fresher with an M.Tech in biotechnology?   \n",
       "85                                                                                                                                  Can height increase after 25?   \n",
       "86                                 What were the major effects of the cambodia earthquake, and how do these effects compare to the Valparaiso earthquake in 1822?   \n",
       "87                                                                                                              What's the difference between honest and sincere?   \n",
       "88                                                                                                                Which is the best gaming laptop under Rs 60000?   \n",
       "89                                                                                             What is your review of The Next Warrior: Proving Grounds - Part 5?   \n",
       "90                                                                     Which book should I choose for reference for physics and chemistry (class 11, CBSE board)?   \n",
       "91  National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?   \n",
       "92                                                                                                            What is the best romantic movie you have ever seen?   \n",
       "93                                                                                                                         What causes nightmares that seem real?   \n",
       "94                                                                                               What are some of the major influences of abstract expressionism?   \n",
       "95                                                                                                                                       How do 3D printing work?   \n",
       "96                                                                                                               Who are some notable folks who attended Caltech?   \n",
       "97                                                                                                                                             What is a Horcrux?   \n",
       "98                                         What are the general requirement to become a Product Manager or a Program Manager in a product based software company?   \n",
       "99                                                                                                       How could I get Skype to work on an android 4.1.1 phone?   \n",
       "\n",
       "    is_duplicate  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              1  \n",
       "6              0  \n",
       "7              1  \n",
       "8              0  \n",
       "9              0  \n",
       "10             0  \n",
       "11             1  \n",
       "12             1  \n",
       "13             1  \n",
       "14             0  \n",
       "15             1  \n",
       "16             1  \n",
       "17             0  \n",
       "18             1  \n",
       "19             0  \n",
       "20             1  \n",
       "21             0  \n",
       "22             0  \n",
       "23             0  \n",
       "24             0  \n",
       "25             0  \n",
       "26             0  \n",
       "27             0  \n",
       "28             0  \n",
       "29             1  \n",
       "..           ...  \n",
       "70             0  \n",
       "71             1  \n",
       "72             1  \n",
       "73             1  \n",
       "74             1  \n",
       "75             0  \n",
       "76             0  \n",
       "77             0  \n",
       "78             0  \n",
       "79             1  \n",
       "80             0  \n",
       "81             0  \n",
       "82             0  \n",
       "83             0  \n",
       "84             1  \n",
       "85             1  \n",
       "86             1  \n",
       "87             0  \n",
       "88             1  \n",
       "89             0  \n",
       "90             0  \n",
       "91             0  \n",
       "92             1  \n",
       "93             1  \n",
       "94             0  \n",
       "95             1  \n",
       "96             0  \n",
       "97             0  \n",
       "98             0  \n",
       "99             0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.drop(['id','qid1','qid2'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGDCAYAAAC2gxMSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xe4XGW59/HvTkILJhAkCCpIUW8RpUgJvMAhIEXhFewC\nwqEoAqLAhQ0pAgqKIHgABZQuxQIcj0cQCK8UQ6QogtK8KVL0cNBNgCS0RGC/fzxrzLDdZXaSme3e\n6/u5rlyZWbPKvWb2zPqt51mlq6enB0mSVF9jhrsASZI0vAwDkiTVnGFAkqSaMwxIklRzhgFJkmrO\nMCBJUs2NG+4C1FkRsThwELAr8BbgOeBW4KuZ+dsO17IJMCYzZyzg9IsBFwI7As8Ab8zMV5peXxV4\nuNdkLwD3A2cBp2fmIju3NiJuAB7MzE9GxJ7A2Zm5SL5jEbED8HBm3rsQ83gQuCgzj14UNfWzjBuA\ntYG1MvN/e712PuUz2rrFeS0HvD8zz21x/FXx826ex6Cfd0S8CfgSsD2wIvAEcCXwzcx8bEGXPVQR\n0QXsDlydmX+LiKnA9cDKmfmXTtVRZ7YM1EhEjAemA/sB3wLWBd4DPAVMj4gtO1zSryiBZEFtA3wM\n+AgwpTkI9LITsBLwemB94GLgJODEhVj2YH4MvGFRzCgi3gBcAaywKObXAZOA0xfBfL4J/PsCTOfn\n3dpyNgB+B6wO7AkEsA/wDuD2iHhXO5ffy/8BLgDGV89/TfkMH+9gDbVmy0C9HAu8lbLX1vwl2zMi\nVgC+ExHvWJR7T4PoWsjpJ1X//2KQmp/KzCeqx/8L3BcRLwHfiohzF2bvqz+Z+QJlr3RRWNj3qdMe\nBt4fETtn5o8WYj4Lut5+3oOIiHGUkHQT8IGmIP1oRFwPTAMuqX4PXmp3PfRa58ycR2mlUIcYBmqi\n6h7YCzinVxBoOACY0NioRsQqwAnAu4ElgV8Ch2Tmn6rXH6E0ix7btIx/DIuIo4GNgRnAp4FlgeuA\nfTLz8WrcscB5EbFnZk7to+bxwFeAnSlNmL8HDsvMX1bzP6oa9ZWIOGaIzd9nAccBHwWOrua3W2a+\nuWn5/xjW1AT9ceBI4E3AbcBnM/OuPmrfk6Zm44iYABwPfJiy9/Nr4MDMzIgYAxwG7FHN93nK+71f\nZnYDf65me31EXJCZe0bEysC3gW0pG6HrKZ/P49XylqS0/uxC+aH95mBvSES8g/KZbwL0UPZOD8nM\nJ6vXe4CvAXtXk2zQtNFtdh2wFHBaRPyyWoe+lvda4OvA/6UEu5uBz2fmHdV7/4mm5a6WmY8Mtg4D\n8PN+tW0pOwYf6d2ilpkvRcThVc3bAr/oq4un97Dq7+ckYHNKa+OVwJcy85nq9R0oOyRvo3TrXQZ8\ngfLdnl7N9uGIOAa4gaZugoF+C5pqeZnS7flxYAngv4F9M3NORIyt3pNdgOWBBL6WmZcO8j7Vht0E\n9bE6ZYN8S18vZubDmfkHgIiYSNmILwdsB0wFlgFujIhlhrDMLYF1gK0pTfrrAV+tXtuQ8uU9GPhg\nP9P/iPLjvS+lS+MW4OqImEL54ftMNd5K1fOWZeazlB/7dw5lOuBk4Iiq/meA/9fie/ITSrDaBdgA\neBa4pjru4RDKcRyfpXSb7AJsBhxeTdtorv0QcFBELE35sXyB0ry6HbA4cF0V+gC+S2ku3xnYgvIZ\nrtFfcdXGbwblR3zzatp1gGurH9KGfSgb7w/2EwQaDgReAU7rZ3ljgWsp7+NHgSnAk5S/sVUpn+cl\nlICwEvM3kAvEz/ufbArMaXzn+3AL8GK1vEFVXRs3An+gfM8/DLwd+M/q9eWrx2dSwsDHKV18X6R8\ntjtVs9qIvr/LA/0WNOxG2cHYpBp3R8r7DGWH5AOU9zSAS4EfRsRqraxfHdgyUB+NJvVnWhh3t2r8\nnTPzKYCI+AjwaPXad1tc5hhgr8ycU83jx5Q9DTKzOyIAZjWW0Swi3g68D9guM6dVgw+qvvyfz8yP\nRMSsal4L2pz4NDBxiNMcl5mXVzXuAfyF8gP8vf4miLKi7wG2zMwbqmGfouwdvpayl7JHZl5dTfJo\nRFzN/A1XY8/6qcycFRGfBJYG9szMl6v57ULZmH4oIq6kHIz1icy8tnp9NwbeoH6a8rexV2b+vZpm\nZ+DeqvYrq/HOz8w7B36LIDNnRsRngJ9ExA8z82e9RtmOstGIzLy/Wt7uwIPAAZn5hYh4AZi3EJ9v\nb37e8y0PzOrvxczsiYingMkDzKPZ/sCfMvMLjQHV389fqgOFX6QEmL9k5qPVOr8HeDYzX66WBdCd\nmc9Wvw2N+Qz4W0A5ZghgJqX15WXg/oi4lhIMAN5MaYF5JDOfiIhjKS09//TbU1eGgfp4svp/uRbG\nfQfwx+aNdGY+GRH3Vq+16olGEKjMovwgtKKxnF/3Gj6dsme6KEyk9CkPxY2NB9UP9X0MvrfZeP22\npmlnAp+rnv48IjaJiOMoey1vA9ZkftNpb+tRfqRnNf9oUpqj16RsUBcDbm9eXnV0eX/eAfymEQSq\nae6LiCer1xph4E8DzONVMvPSiPhP4IyI+FUfy5vZCALV+PMi4laG9jc2FH7e8z0JLBsRXX0dbxPl\n6P5laG3noVHjehHxbB+vrQmcR2ktuSIi/kI5JuG/KF1Rg2n1t+ChRliqzGL+QZ2nU1og/ycifgtc\nDVycmf0Gorqxm6A+HgL+RunH/ycRMTUi/jsiVqL/A6HGAn/v5zX453A5t49xWj04akFraEnVBxnA\nHQOM1ldY7r3ssZTm8IEMWG9EHEFpMp8IXEU5gv6iASaZB9xDaS5t/vdW4FRKfz/883s9b4B5tvp+\nD/UguU9T+m+/vYDLWyT8vP/JTcBrqvn0ZX1Ka8StA8yj+f2aR9nA967xLcDlmdmTmR8D1gJOAVYB\nfgq0ctpoq38r/f7eZGZSukp3oHSH7QLcExHvbmH5tWAYqInqIKHzgb0j4vXNr1V7AYdS9lCeoDQN\nvy3Ked6NcZan/Jg2jsSeR1OTa3WcweuGWNZAZwA0lrNpr+GbNr22MD5J+TH5cfV8HjCh1zh9nfa4\nQeNBRExi8A0MwH19TDsxIv4WEZtRzvP+SmZ+NjPPyczfVctu/Lj3fp/uAVaj7Fk/mJkPUoLeyZS9\n0j9Sfhj/0d9bHdD21gFqvBfYsOrTbkzzdkp30QK/35n5V8pxIXtQjkVoXt5ro2lXt+r/3rBpeYvy\nrBY/71e7tprua9UBjUTE9hFxR0RsTznQ7xHm77m/6vteaX6/7qG0ADzaVOPLwH8AK0fEBhFxcmbe\nm5nfysxtKN0mH+tnnZst9G9BRHwa+FBmXp2Zh1B+65JyDIGwm6Buvkbps7+pOlr4VsoG/POUg462\nqfoKL6YczPSjiDi0mvYESp9r41Sxm4FdIuKnwOxq3kM9BWkO8PaIWCEz/9b8QmY+FBE/ojQx7wc8\nBnyKssdy8BCXs1xErEj5sZ0EvJfyY/eNzHyoaX2Oi4iDKc2X763+/a3XvL4eEX+lnP98PKW59ScD\nLTwz74+InwGnR8T+lD7h4yjNmL+h9O1uFxG/oGyw9qf0dTb2yhpdLWtHxF2UU8IOp/THf5nSH3s8\n5eCre6o+1zOBYyPiCUqr0FeZfw53X75DOaDtvIj4RvU+nUY5avuXA63fYDLzwoj4GGWvrHFRoOso\n7/klEXFg9V4cRjnI9ftN6/2G6iCvP2frp7j5eQ/yeVdnDOxC2Zu/KiK+DjwA3Mn8LqH3ZDnFr/F+\n7V0dB3ArpTXjncxvuv8O5YDe8yPieEpr0Hcpn+f9lLMmDoiIF4FzKEHsfX2s83oR8XSvWhfFb8Hy\nwDFVN8ZdlIM0V6O9154YUWwZqJHqiOp/oxylfRRwN+UI3zHAJpl5UzXei5QDvOZSLgx0HeWHbPPG\naUKUH+7GhuJaStPbUK8keDylGfmafl7fh9K3dxHl4ihTgG0z8+YhLudnlL7ixynr837KKY5HNkbI\nzOsp78mhlL2NrZl/6mKz71N+5G6lbGy2zMznWqhhT0of8s+qaRen/NjOpfywLkvZ47yWcpDZoZSg\nND4zZ1M2zN+knL72AuXsjOcpn80MSrDfqilUfZ7SBHsO5cjrxxigybfag98GeCPwW8oG8g5g6+bj\nCBbCvjQdsFb1U3+Asnd6ZVXjayl/Y43jEs6jbCzvo/RJt8rPe5DPu3oP7qRsFB+kXPDnLsoZEOdS\nmvB/EBGN00gvovS7n0753q9M2etvzOsJynu4YrXca6oatsnMeZn5AOVz2IZyxsH1lIMxd6lmcS9w\nOWVn45g+yl3Y34KvU96b0ynh5ETgqMy8oMXpR72unp5OXV9GGrli/nnnmzdCk0YvP2+IckXS5Rpn\nU2h0s5tA0r+86piVgX6vXs5+Lm6kBVO1nqgmDAOSRoJbGPgiOv9D6eKQtADsJpAkqeY8gFCSpJoz\nDEiSVHO1PWagu3uO/SMj2KRJ43n66eeHuwypdvzujVyTJ0/o9wqwtgxoRBo3buzgI0la5PzujU6G\nAUmSas4wIElSzRkGJEmqOcOAJEk1ZxiQJKnmDAOSJNWcYUCSpJozDEiSVHOGAUmSas4wIElSzRkG\nJEmqOcOAJEk1ZxiQJKnmansL49Fu7+OvG+4StIDOPXSr4S5BUs3YMiBJUs0ZBiRJqrm2dBNExGLA\nucCqwBLAscCfgSuAB6rRzsjMH0fEPsC+wEvAsZl5RUQsBVwErADMAfbIzO6I2Bg4pRp3WmYeUy3v\nKGCHavjBmXlbO9ZLkqTRqF3HDOwGzMzM3SNiOeBO4KvAyZl5UmOkiFgROBDYAFgSuCkirgX2B+7K\nzKMjYmfgCOAg4EzgQ8CfgCsjYj2gC9gCmAKsDFwObNim9ZIkadRpVxi4FLisetxF2WNfH4iI2InS\nOnAwsBEwIzPnAnMj4kFgbWAz4IRq+quAIyNiIrBEZj5EmdE1wNbAXEorQQ/wWESMi4jJmdndpnWT\nJGlUaUsYyMxnASJiAiUUHEHpLjg7M2+PiMOBoygtBrOaJp0DLANMbBrePGx2r3FXB14EZvYxjwHD\nwKRJ4xk3buyCrJ7UVpMnTxjuEqQB+Tc6+rTt1MKIWBn4KXB6Zl4SEctm5jPVyz8FTgN+BTT/VU0A\nnqFs9CcMMKx5+Lx+hg/o6aefH+oqSR3R3T1nuEuQ+jV58gT/RkeogUJcW84miIjXAdOAL2XmudXg\nayJio+rxu4HbgduAzSNiyYhYBlgTuBuYAWxfjfteYHpmzgbmRcQaEdEFbAdMr8bdLiLGRMQqwJjM\nfLId6yVJ0mjUrpaBw4BJlL7+I6thhwDfjoi/A08An8rM2RFxKmWjPgY4PDNfjIgzgAsi4ibKnv+u\n1Tz2Ay4GxlKOE7gVICKmAzdX8zigTeskSdKo1NXT0zPcNQyL7u45o3rFvQLhyOUVCPWvzG6CkWvy\n5Ald/b3mRYckSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5\nw4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYM\nA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMM\nSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAg\nSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4Ak\nSTVnGJAkqeYMA5Ik1ZxhQJKkmhvXjplGxGLAucCqwBLAscC9wPlAD3A3cEBmvhIR+wD7Ai8Bx2bm\nFRGxFHARsAIwB9gjM7sjYmPglGrcaZl5TLW8o4AdquEHZ+Zt7VgvSZJGo3a1DOwGzMzMzYH3AN8B\nTgaOqIZ1ATtFxIrAgcCmwHbANyJiCWB/4K5q3B8AR1TzPRPYFdgMmBIR60XEu4AtgCnAzsB327RO\nkiSNSu0KA5cCR1aPuyh77OsDN1bDrgK2BjYCZmTm3MycBTwIrE3Z2F/dPG5ETASWyMyHMrMHuKaa\nx2aUVoKezHwMGBcRk9u0XpIkjTpt6SbIzGcBImICcBllz/5b1UYcStP/MsBEYFbTpH0Nbx42u9e4\nqwMvAjP7mEf3QDVOmjSecePGDnXVpLabPHnCcJcgDci/0dGnLWEAICJWBn4KnJ6Zl0TECU0vTwCe\noWzcJwwyfLBx5/UzfEBPP/38UFZH6pju7jnDXYLUr8mTJ/g3OkINFOLa0k0QEa8DpgFfysxzq8F3\nRMTU6vF7genAbcDmEbFkRCwDrEk5uHAGsH3zuJk5G5gXEWtERBflGIPp1bjbRcSYiFgFGJOZT7Zj\nvSRJGo3a1TJwGDAJODIiGscOHAScGhGLA/cBl2XmyxFxKmWjPgY4PDNfjIgzgAsi4ibKnv+u1Tz2\nAy4GxlKOE7gVICKmAzdX8zigTeskSdKo1NXT0zP4WKNQd/ecUb3iex9/3XCXoAV07qFbDXcJUr/s\nJhi5Jk+e0NXfa150SJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1Zxh\nQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSaq5lsJARCwXEVtXj78cEZdGxNvbW5okSeqEVlsGfgi8\nrQoEHwH+GzizbVVJkqSOaTUMTMrM7wA7Aedn5oXA+PaVJUmSOmVci+ONiYj1gfcDW0TEukOYVpIk\n/QtrtWXgS8CJwEmZ+SdKF8EhbatKkiR1TEt795n5S+CXTc83bltFkiSpo1oKAxGxB3ASMKl5eGaO\nbUdRkiSpc1rt9z8KmJqZd7ezGEmS1HmtHjPwPwYBSZJGp1ZbBm6PiMuAacCLjYGZ+YO2VCVJkjqm\n1TCwDDAH2KRpWA9gGJAkaYRr9WyCvSJiMSCqae7OzJfaWpkkSeqIVu9NsD7wAHABcB7wWERMaWdh\nkiSpM1rtJjgV+Fhm3goQERsDpwEbtaswSZLUGa2eTfCaRhAAyMxbgCXbU5IkSeqkVsPAUxGxU+NJ\nRLwfmNmekiRJUie12k2wL3BhRJwLdAEPAru3rSpJktQxrZ5NcD8wJSKWBsZk5pz2liVJkjplwDAQ\nEd/PzE9FxPWU6wo0hgOQmVu1tzxJktRug7UMfK/6/+g21yFJkobJgGEgM2+vHn44Mz/b/FpEXADc\n2K7CJElSZwzWTXA2sDqwQUSs1fTSYpRLFEuSpBFusG6CY4FVgVOAY5qGvwTc16aaJElSBw14nYHM\nfCQzb8jMdYD7M/NG4BVgXWBuJwqUJEnt1eq9Cc4AjoiItwOXAO/COxZKkjQqtHoFwo2AzwAfBc7J\nzE8Aq7StKkmS1DGthoGx1bg7AVdFxHhg6bZVJUmSOqbVMPAD4H+BR6obFt3O/GsQSJKkEaylMJCZ\nJwMrZeYHqkGbZ+Yp7StLkiR1SqsHEL4JuDoiHoiIlYCfRMSqba1MkiR1RKvdBN8DTgSeBZ4Afohn\nE0iSNCq0GgaWz8xpAJnZk5lnARPbV5YkSeqUVsPACxHxRqo7F0bEZnjRIUmSRoXBLkfccAhwBbBG\nRNwJLEe55oAkSRrhWgoDmfmbiNgQeCvlmgN/zMx5ba1MkiR1REthICLOo+oiaBpGZu7dlqokSVLH\ntNpNcEPT48WAHYE/LvJqJElSx7XaTXBB8/OIOAeY0ZaKJElSR7V6NkFvawIrLcpCJEnS8Gj1mIFX\nKMcMdFWDuoEvt6soSZLUOa12EyxQC0JETAG+mZlTI2I9yumJD1Qvn5GZP46IfYB9gZeAYzPziohY\nCrgIWAGYA+yRmd0RsTFwSjXutMw8plrOUcAO1fCDM/O2BalXkqQ6GjQMRMQkYD9gCuW0wt8AZ1A2\nvo9m5vX9TPdFYHfguWrQ+sDJmXlS0zgrAgcCGwBLAjdFxLXA/sBdmXl0ROwMHAEcBJwJfAj4E3Bl\nFTC6gC2q+lYGLgc2HMJ7IElSrQ0YBqobFN0E/BqYBiwFbAzcCfwV2HyAyR8CPghcWD1fv8wydqK0\nDhwMbATMyMy5wNyIeBBYG9gMOKGa7irgyIiYCCyRmQ9VtV0DbE25EuK0zOwBHouIcRExOTO7W38b\nJEmqr8FaBk4EDs3Mi5sHRsTVwAuZ+Vzfk0FmXt7rzoa3AWdn5u0RcThwFCVUzGoaZw6wDOW+B7P6\nGDa717irAy8CM/uYx4BhYNKk8YwbN3agUaRhMXnyhOEuQRqQf6Ojz2Bh4G2Z+arLDkfEZOB5yhkF\nQ/HTzHym8Rg4DfgV0PxXNQF4hrLRnzDAsObh8/oZPqCnn35+iOVLndHdPWe4S9BC2Pv464a7BC2E\ncw/darhLaJuBQtxgBwYu3ntAZnZn5gdbmLa3ayJio+rxu4HbKa0Fm0fEkhGxDCVg3E25hsH21bjv\nBaZn5mxgXkSsERFdwHbA9Grc7SJiTESsAozJzCeHWJskSbU12Ab97oj4WO+B1UF99wxxWfsD346I\nG4BNKWcOPAGcStmoXwccnpkvUg5QXCsibgI+BRxTzWM/4GJKiLgjM2/NzNur6W+mHDx4wBDrkiSp\n1rp6enr6fTEiVqNsaG+kbIChHEC4KfBvmflIuwtsl+7uOf2v+ChgU+XINZqbKevA797INpq/f5Mn\nT+jq77UBWwYy82FgHeBeYKvq3z3AeiM5CEiSpPkGvc5AZs4EjutALZIkaRgs6L0JJEnSKGEYkCSp\n5gwDkiTV3GCXI27crbDh78ArwBLA7Myc1MbaJElSBwx2NsGYzBwLfB/YA1gqM8cDHwUu60B9kiSp\nzVrtJpiSmRdVNwMiM70zoCRJo8SgpxZWnouIvYCfUALE7rz65kCSJGmEarVlYDfK7YifAP5CubfA\n7u0qSpIkdU5LLQOZ+SjwvohYLjOfanNNkiSpg1oKAxGxLvAjYHxEbEy59fBHM/N37SxOkiS1X6vd\nBKcCHwBmZubjlDsQntm2qiRJUse0GgbGZ+Z9jSeZeS3lWgOSJGmEazUMPBUR61BdgCgiPg547IAk\nSaNAq6cW7g9cAKwVEc8ADwAfb1tVkiSpY1oNA9tk5mYRsTQwNjNnt7MoSZLUOa2Ggc8AZ2bmc+0s\nRpIkdV6rYeDPEXEdcCvwQmNgZn61LVVJkqSOaTUM3NL0uKsdhUiSpOHR6hUIj2l3IZIkaXgMGAYi\n4neZ+a6IeIXqtMJKF9BT3d5YkiSNYAOGgcx8V/V/q9cjkCRJI0yr9yZYgXJdgddQWgXGAqtl5r+3\nsTZJktQBre7x/yewLuVWxksDOwKvtKsoSZLUOa2GgeUzcw/g55RgMBVYq11FSZKkzmk1DDxd/Z/A\nOpk5C1isPSVJkqROavU6A9dFxKXA54FpEfEu4MX2lSVJkjqlpZaBzDwcODQzHwV2obQQfLCdhUmS\npM4Y7DoDY4ADgLcCNwEPZebvgN91oDZJktQBg7UMnA58BHgOOCwivtL+kiRJUicNFga2ALbIzEOB\nrYAPtb8kSZLUSYOFgRczswcgM2fy6ksSS5KkUWCwMNB74++FhiRJGmUGO7XwTRFxbn/PM3Pv9pQl\nSZI6ZbAwcEiv5ze2qxBJkjQ8Brtr4QWdKkSSJA0Pb00sSVLNDRgGImLpThUiSZKGx2AtAzcARMTp\n7S9FkiQNh8EOIHxNRFwEvCciluz9omcTSJI08g0WBrYFtgQ2xzMJJEkalQY7m+DPwA8i4vfAvUBU\n09ydmS91oD5JktRmrZ5NsBjwAHABcB7wWERMaVtVkiSpYwbrJmg4BfhYZt4KEBEbA6cBG7WrMEmS\n1Bmttgy8phEEADLzFuCfDiiUJEkjT6th4KmI2KnxJCLeD8xsT0mSJKmTWu0m+BRwUUScA3QBDwG7\nta0qSZLUMS2Fgcx8AJhSXZFwTGbOaW9ZkiSpU1ptGQAgM59rVyGSJGl4eKMiSZJqrqUwEBFr9TFs\n40VfjiRJ6rQBuwkiYlNgLHB2RHyCcvBgY7ozgbe2tzxJktRugx0zsA2wBbAS8NWm4S8B3xts5tVV\nCr+ZmVMj4s3A+UAPcDdwQGa+EhH7APtW8zw2M6+IiKWAi4AVgDnAHpnZXbVGnFKNOy0zj6mWcxSw\nQzX84My8raW1lyRJg96b4GiAiNg9My8cyowj4ovA7kDjoMOTgSMy84aIOBPYKSJuBg4ENqBcxOim\niLgW2B+4KzOPjoidgSOAgyitER8C/gRcGRHrUVortgCmACsDlwMbDqVWSZLqrNWzCX4VEScCyzG/\nq2CwWxg/BHwQaISI9Zl/58OrKHdEfBmYkZlzgbkR8SCwNrAZcELTuEdGxERgicx8CCAirgG2BuZS\nWgl6KPdMGBcRkzOzu8V1kySp1loNAz8Bplf/elqZIDMvj4hVmwZ1VRtsKE3/ywATgVlN4/Q1vHnY\n7F7jrg68yKuvhtgYf8AwMGnSeMaNG9vKqkgdNXnyhOEuQaqtun7/Wg0Di2Xm5xdyWa80PZ4APEPZ\nuE8YZPhg487rZ/iAnn76+aFVL3VId7fX9JKGy2j+/g0UdFq9zsBNEfG+iFh8Ieq4IyKmVo/fS2ll\nuA3YPCKWjIhlgDUpBxfOALZvHjczZwPzImKNiOgCtqvmMQPYLiLGRMQqlCskPrkQdUqSVCuttgx8\nGPgMQEQ0hvVk5lDa2T8HnFUFivuAyzLz5Yg4lbJRHwMcnpkvRsQZwAURcRNlz3/Xah77ARdTTnec\n1nRL5enAzdU8DhhCTZIk1V5XT09LhwCMOt3dc0b1iu99/HXDXYIW0LmHbjXcJWgh+N0b2Ubz92/y\n5Ald/b3WUstARHylr+GZ+dW+hkuSpJGj1WMGupr+LQ7sCLyuXUVJkqTOafUWxsc0P4+IrwHT2lKR\nJEnqqAW9a+FrgFUWZSGSJGl4tHrMwMPMv9jQGGBZ4MR2FSVJkjqn1VMLpzY97gGeqc77lyRJI1yr\n3QSPUS4CdBJwKrBnRCxoF4MkSfoX0mrLwAnAW4BzKWcU7EW5L8DBbapLkiR1SKthYFtgvcx8BSAi\nrgTualusbpQaAAAJq0lEQVRVkiSpY1pt6h/Hq4PDOMrthyVJ0gjXasvAxcANEfHD6vkuwCXtKUmS\nJHVSSy0Dmfl14GuUawusChxXDZMkSSPcoC0DETEJGJuZVwFXVbchvqfdhUmSpM4YsGUgItYD7gU2\naBq8DXBnRKzdzsIkSVJnDNZN8C1gl8y8ujEgMw8H9gZObmdhkiSpMwYLA5My84beAzPzGmD5tlQk\nSZI6arAwsFhfVxqshi3enpIkSVInDRYGbgSO6mP4EcBvF305kiSp0wY7m+DLwC8i4uPAbyiXIn4X\n8DdgxzbXJkmSOmDAMJCZcyLi34AtgfWAV4DvZub0ThQnSZLab9DrDGRmD3Bd9U+SJI0y3oZYkqSa\nMwxIklRzhgFJkmrOMCBJUs0ZBiRJqjnDgCRJNWcYkCSp5gwDkiTVnGFAkqSaMwxIklRzhgFJkmrO\nMCBJUs0ZBiRJqjnDgCRJNWcYkCSp5gwDkiTVnGFAkqSaMwxIklRzhgFJkmrOMCBJUs0ZBiRJqjnD\ngCRJNWcYkCSp5gwDkiTVnGFAkqSaMwxIklRzhgFJkmrOMCBJUs0ZBiRJqjnDgCRJNWcYkCSp5gwD\nkiTV3LhOLzAifgfMrp4+DBwHnA/0AHcDB2TmKxGxD7Av8BJwbGZeERFLARcBKwBzgD0yszsiNgZO\nqcadlpnHdHKdJEkayTraMhARSwJdmTm1+rcXcDJwRGZuDnQBO0XEisCBwKbAdsA3ImIJYH/grmrc\nHwBHVLM+E9gV2AyYEhHrdXK9JEkayTrdMrAOMD4iplXLPgxYH7ixev0qYFvgZWBGZs4F5kbEg8Da\nlI39CU3jHhkRE4ElMvMhgIi4BtgauKMzqyRJ0sjW6TDwPPAt4GzgLZQNeldm9lSvzwGWASYCs5qm\n62t487DZvcZdfbBCJk0az7hxYxd4RaR2mTx5wnCXINVWXb9/nQ4D9wMPVhv/+yNiJqVloGEC8Axl\n4z5hkOGDjTugp59+fgFXQWqv7u45w12CVFuj+fs3UNDp9NkEewMnAUTE6yl79dMiYmr1+nuB6cBt\nwOYRsWRELAOsSTm4cAawffO4mTkbmBcRa0REF+UYg+kdWh9Jkka8TrcMnAOcHxE3Uc4e2Bt4Ejgr\nIhYH7gMuy8yXI+JUykZ9DHB4Zr4YEWcAF1TTz6McNAiwH3AxMJZyNsGtHV0rSZJGsI6Ggcxs3oA3\n26KPcc8Czuo17HngI32Mewuw8SIqU5KkWvGiQ5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYk\nSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAk\nqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKk\nmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJq\nzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNGQYkSao5\nw4AkSTVnGJAkqeYMA5Ik1ZxhQJKkmjMMSJJUc4YBSZJqzjAgSVLNjRvuAhaViBgDnA6sA8wFPpmZ\nDw5vVZIk/esbTS0D7weWzMxNgEOBk4a5HkmSRoTRFAY2A64GyMxbgA2GtxxJkkaGrp6enuGuYZGI\niLOByzPzqur5Y8DqmfnS8FYmSdK/ttHUMjAbmND0fIxBQJKkwY2mMDAD2B4gIjYG7hreciRJGhlG\nzdkEwE+BbSLi10AXsNcw1yNJ0ogwao4ZkCRJC2Y0dRNIkqQFYBiQJKnmRtMxAxrlvMqkNLwiYgrw\nzcycOty1aNGyZUAjiVeZlIZJRHwROBtYcrhr0aJnGNBI4lUmpeHzEPDB4S5C7WEY0EgyEZjV9Pzl\niLCrS+qAzLwc+Ptw16H2MAxoJPEqk5LUBoYBjSReZVKS2sAmVo0kXmVSktrAKxBKklRzdhNIklRz\nhgFJkmrOMCBJUs0ZBiRJqjnDgCRJNeephdIoFBGrAvcD91aDlgL+AHwmM/+6gPPsycyuiNgPIDPP\nHOL0qwFHZOYnFmT5LS5jVeBhYNvMvLZp+CPA1Mx8ZIBpr8/MLdtVm/SvzJYBafR6PDPXzcx1gbcB\nDwKXLexMM/PMoQaBypuANRZ2+S34O3BWREwYdMxXm9qGWqQRwZYBqQYysycijgL+GhFrA8sBRzdu\nRRsR5wM3VP9+TrkpzVuAR4HdMvOpxrwi4uhqnkdHxK7AEUAP8BtgH2AF4BxgWWAl4IeZeShwKrB6\nRHw3Mw+IiEOBjwJjgWuAL2Xmqy58EhF7AZ+r5n87pWXj2Yjorp6vCGyYmc3XzH8cuJZyV8tP9X4v\nIuIwYDfgZWAa8EXg29Vrt2bmlFbfV2m0sGVAqonMnAc8QGklGMg7gP/IzLWA+4Cj+xopIt5A2Yhu\nW407FtgB2IUSADYG1gY+HRHLAwcCv62CwHuA9YENgfWANwAf7zX/dwKHA1tk5juB54CjqpeXB46v\nWj76unnO54DtImKbXvPcHtixWvZ6wJuB/TLzwOo9MgiolmwZkOqlB3hhkHHuz8wbqscXAJf0M94m\nwIzM/AtAZu7eeCEitoyIz1OCxeLA0r2m3RqYQtm7h3JMw2O9xtkC+Hlmzqyefx84r+n1W/tbgcyc\nHRH7ULoL3tn00laUoPJCVee5wB7Ad/ubl1QHhgGpJiJicSAoBxW+kXJ/h4bFmh433wlyTK/nzV61\nRx4Rk6uHhwKrU0LEf1E2/F2vnpSxlNaHk6tpl+1jOb1bLrto+s1qbND7k5nTIqLRXdDSPKW6sptA\nqoGIGAMcA9ySmQ8BT1L675eMiOWAzV89eqxbPd4LuKqf2f4GmBIRK1bPvw3sBGwDnJiZlwIrU7oA\nxlI29o0N73XA7hHxmogYRwkNH+41/xuAHav6oByPcP3Q1rx0FwCvb1ruLhGxVLXcvZrm+XI1TKod\nw4A0er0+Iu6MiDuB31M2yrsCZOY9wJXAPcClwPSm6Z4CjomIeygHAx7b18wz83HgIOCaiLib0v1w\nHvAN4MKIuB34AvBbYDXK8QfLRsSFmflz4HJKU//dwJ2ULonm+f+hmteNEfFHygGJRwzlDcjM2ZQQ\nsVj1/ArgiqqmeygHSJ5Wjf4z4PcRseRQliGNBt61UNI/VOfp35CZqw5zKZI6yJYBSZJqzpYBSZJq\nzpYBSZJqzjAgSVLNGQYkSao5w4AkSTVnGJAkqeYMA5Ik1dz/B29WU3n+U0ZXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14036fcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "df.groupby('is_duplicate').is_duplicate.count().plot(kind='bar', rot=0)\n",
    "plt.xlabel('Duplicate or Not')\n",
    "plt.ylabel('Count of Paired Questions')\n",
    "plt.title('Count of Duplicated or Not_Duplicated Questions', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63219471171683694"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['question1', 'question2']]\n",
    "y = df.is_duplicate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "# Null Model for df_test\n",
    "y_test.value_counts()[0]/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Get stopwords_set and punctuation set.\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Get unigrams for each sentence.\n",
    "def get_unigram_sentence(sentence):\n",
    "    return [word for word in word_tokenize(sentence.lower()) if word not in stopwords_set and\\\n",
    "            word not in punctuation]\n",
    "\n",
    "# Function to get unigrams of question1 and question2.\n",
    "def get_unigrams(df):\n",
    "    df['question1_unigram'] = df['question1'].apply(lambda x:\\\n",
    "                                             get_unigram_sentence(x.decode(encoding='utf-8')))\n",
    "    df['question2_unigram'] = df['question2'].apply(lambda x:\\\n",
    "                                             get_unigram_sentence(x.decode(encoding='utf-8')))\n",
    "# Get unigrams of question1 and question2 of training dataset.\n",
    "get_unigrams(df_train)\n",
    "\n",
    "# Function to get common_unigram_ratio.\n",
    "def get_common_unigram_ratio(df):\n",
    "    df['common_unigram_count'] = df.apply(lambda x: len(set(x['question1_unigram']).\\\n",
    "                                        intersection(set(x['question2_unigram']))), axis=1)\n",
    "    \n",
    "    df['unigram_count'] = df.apply(lambda x: max(len(set(x['question1_unigram']).\\\n",
    "                                   union(set(x['question2_unigram']))), 1), axis=1)\n",
    "    \n",
    "    df['common_unigram_ratio'] = df['common_unigram_count'] / df['unigram_count']\n",
    "\n",
    "# Get common_unigram_ratio of training dataset.\n",
    "get_common_unigram_ratio(df_train)\n",
    "\n",
    "\n",
    "# Function to get bigrams of question1 and question2.\n",
    "def get_bigrams(df):\n",
    "    df['question1_bigram'] = df['question1_unigram'].apply(lambda x: [i for i in ngrams(x, 2)])\n",
    "    df['question2_bigram'] = df['question2_unigram'].apply(lambda x: [i for i in ngrams(x, 2)])\n",
    "    \n",
    "# Get bigrams of question1 and question2 of training dataset.\n",
    "get_bigrams(df_train)\n",
    "\n",
    "\n",
    "# Function to get common_bigram_ratio.\n",
    "def get_common_bigram_ratio(df):\n",
    "    df['common_bigram_count'] = df.apply(lambda x: len(set(x['question1_bigram']).\\\n",
    "                                         intersection(set(x['question2_bigram']))), axis=1)\n",
    "\n",
    "    df['bigram_count'] = df.apply(lambda x: max(len(set(x['question1_bigram']).\\\n",
    "                                union(set(x['question2_bigram']))), 1), axis=1)\n",
    "\n",
    "    df['common_bigram_ratio'] = df['common_bigram_count'] / df['bigram_count']\n",
    "    \n",
    "    \n",
    "# Get common_bigram_ratio of training dataset.\n",
    "get_common_bigram_ratio(df_train)\n",
    "\n",
    "# Function to get trigrams of question1 and question2.\n",
    "def get_trigrams(df):\n",
    "    df['question1_trigram'] = df['question1_unigram'].apply(lambda x: [i for i in ngrams(x, 3)])\n",
    "    df['question2_trigram'] = df['question2_unigram'].apply(lambda x: [i for i in ngrams(x, 3)])\n",
    "\n",
    "# Get trigrams of question1 and question2 of traning dataset.\n",
    "get_trigrams(df_train)\n",
    "\n",
    "# Function to get common_trigram_ratio of question1 and question2.\n",
    "def get_common_trigram_ratio(df):\n",
    "    df['common_trigram_count'] = df.apply(lambda x: len(set(x['question1_trigram']).\\\n",
    "                                          intersection(set(x['question2_trigram']))), axis=1)\n",
    "\n",
    "    df['trigram_count'] = df.apply(lambda x: max(len(set(x['question1_trigram']).\\\n",
    "                                   union(set(x['question2_trigram']))), 1), axis=1)\n",
    "\n",
    "    df['common_trigram_ratio'] = df['common_trigram_count'] / df['trigram_count']\n",
    "    \n",
    "# Get common_trigram_ratio of question1 and question2 of training dataset.\n",
    "get_common_trigram_ratio(df_train)\n",
    "\n",
    "\n",
    "# Make tfidf_matrix of paired quesions.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords_set)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit(df_train.question1.values + df_train.question1.values)\n",
    "\n",
    "# Function to get cosine_similarity of each paired questions.\n",
    "def get_cosine_similarity(df):\n",
    "    tfidf_matrix_transform1 = tfidf_vectorizer.transform(df.question1.values)\n",
    "    tfidf_matrix_transform2 = tfidf_vectorizer.transform(df.question2.values)\n",
    "    cos = []\n",
    "    for i in range(len(df)):\n",
    "        cos.append(cosine_similarity(tfidf_matrix_transform1[i].toarray(),\\\n",
    "                                     tfidf_matrix_transform2[i].toarray()))\n",
    "\n",
    "    df['cosine_similarity'] = cos\n",
    "    df['cosine_similarity'] = df.cosine_similarity.apply(lambda x: x[0][0])\n",
    "\n",
    "# Get cosine_similarity of each paired quesions of training dataset.\n",
    "get_cosine_similarity(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_unigram</th>\n",
       "      <th>question2_unigram</th>\n",
       "      <th>common_unigram_count</th>\n",
       "      <th>unigram_count</th>\n",
       "      <th>common_unigram_ratio</th>\n",
       "      <th>question1_bigram</th>\n",
       "      <th>question2_bigram</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>common_bigram_count</th>\n",
       "      <th>bigram_count</th>\n",
       "      <th>common_bigram_ratio</th>\n",
       "      <th>question1_trigram</th>\n",
       "      <th>question2_trigram</th>\n",
       "      <th>common_trigram_count</th>\n",
       "      <th>trigram_count</th>\n",
       "      <th>common_trigram_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>When do I start preparing for the UPSC exam?</td>\n",
       "      <td>Where from and how should I start preparing for the UPSC?</td>\n",
       "      <td>1</td>\n",
       "      <td>[start, preparing, upsc, exam]</td>\n",
       "      <td>[start, preparing, upsc]</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[(start, preparing), (preparing, upsc), (upsc, exam)]</td>\n",
       "      <td>[(start, preparing), (preparing, upsc)]</td>\n",
       "      <td>0.887933</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[(start, preparing, upsc), (preparing, upsc, exam)]</td>\n",
       "      <td>[(start, preparing, upsc)]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254499</th>\n",
       "      <td>What resources would you give to an American adult, who has never studied a language before, to begin studying Italian?</td>\n",
       "      <td>What resources would you give to an American adult, who has never studied a language before, to begin studying Spanish?</td>\n",
       "      <td>0</td>\n",
       "      <td>[resources, would, give, american, adult, never, studied, language, begin, studying, italian]</td>\n",
       "      <td>[resources, would, give, american, adult, never, studied, language, begin, studying, spanish]</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[(resources, would), (would, give), (give, american), (american, adult), (adult, never), (never, studied), (studied, language), (language, begin), (begin, studying), (studying, italian)]</td>\n",
       "      <td>[(resources, would), (would, give), (give, american), (american, adult), (adult, never), (never, studied), (studied, language), (language, begin), (begin, studying), (studying, spanish)]</td>\n",
       "      <td>0.879002</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>[(resources, would, give), (would, give, american), (give, american, adult), (american, adult, never), (adult, never, studied), (never, studied, language), (studied, language, begin), (language, b...</td>\n",
       "      <td>[(resources, would, give), (would, give, american), (give, american, adult), (american, adult, never), (adult, never, studied), (never, studied, language), (studied, language, begin), (language, b...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      question1  \\\n",
       "5001                                                                               When do I start preparing for the UPSC exam?   \n",
       "254499  What resources would you give to an American adult, who has never studied a language before, to begin studying Italian?   \n",
       "\n",
       "                                                                                                                      question2  \\\n",
       "5001                                                                  Where from and how should I start preparing for the UPSC?   \n",
       "254499  What resources would you give to an American adult, who has never studied a language before, to begin studying Spanish?   \n",
       "\n",
       "        is_duplicate  \\\n",
       "5001               1   \n",
       "254499             0   \n",
       "\n",
       "                                                                                    question1_unigram  \\\n",
       "5001                                                                   [start, preparing, upsc, exam]   \n",
       "254499  [resources, would, give, american, adult, never, studied, language, begin, studying, italian]   \n",
       "\n",
       "                                                                                    question2_unigram  \\\n",
       "5001                                                                         [start, preparing, upsc]   \n",
       "254499  [resources, would, give, american, adult, never, studied, language, begin, studying, spanish]   \n",
       "\n",
       "        common_unigram_count  unigram_count  common_unigram_ratio  \\\n",
       "5001                       3              4              0.750000   \n",
       "254499                    10             12              0.833333   \n",
       "\n",
       "                                                                                                                                                                                  question1_bigram  \\\n",
       "5001                                                                                                                                         [(start, preparing), (preparing, upsc), (upsc, exam)]   \n",
       "254499  [(resources, would), (would, give), (give, american), (american, adult), (adult, never), (never, studied), (studied, language), (language, begin), (begin, studying), (studying, italian)]   \n",
       "\n",
       "                                                                                                                                                                                  question2_bigram  \\\n",
       "5001                                                                                                                                                       [(start, preparing), (preparing, upsc)]   \n",
       "254499  [(resources, would), (would, give), (give, american), (american, adult), (adult, never), (never, studied), (studied, language), (language, begin), (begin, studying), (studying, spanish)]   \n",
       "\n",
       "        cosine_similarity  common_bigram_count  bigram_count  \\\n",
       "5001             0.887933                    2             3   \n",
       "254499           0.879002                    9            11   \n",
       "\n",
       "        common_bigram_ratio  \\\n",
       "5001               0.666667   \n",
       "254499             0.818182   \n",
       "\n",
       "                                                                                                                                                                                              question1_trigram  \\\n",
       "5001                                                                                                                                                        [(start, preparing, upsc), (preparing, upsc, exam)]   \n",
       "254499  [(resources, would, give), (would, give, american), (give, american, adult), (american, adult, never), (adult, never, studied), (never, studied, language), (studied, language, begin), (language, b...   \n",
       "\n",
       "                                                                                                                                                                                              question2_trigram  \\\n",
       "5001                                                                                                                                                                                 [(start, preparing, upsc)]   \n",
       "254499  [(resources, would, give), (would, give, american), (give, american, adult), (american, adult, never), (adult, never, studied), (never, studied, language), (studied, language, begin), (language, b...   \n",
       "\n",
       "        common_trigram_count  trigram_count  common_trigram_ratio  \n",
       "5001                       1              2                   0.5  \n",
       "254499                     8             10                   0.8  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69542322712919014"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Models\n",
    "feature_cols = ['common_unigram_ratio', 'common_bigram_ratio','common_trigram_ratio',\\\n",
    "                'cosine_similarity']\n",
    "df_train_X = df_train[feature_cols]\n",
    "df_train_y = df_train.is_duplicate\n",
    "\n",
    "# Random Forest Clssifier Model\n",
    "rfc_grid = {'max_depth': [3, 5],\n",
    "            'n_estimators': [100, 300],\n",
    "            'max_features': ['sqrt']}\n",
    "rfc_gridsearch = GridSearchCV(RandomForestClassifier(),\n",
    "                                 rfc_grid,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv = 3,\n",
    "                                 scoring='accuracy')\n",
    "rfc_gridsearch.fit(df_train_X, df_train_y)\n",
    "\n",
    "rfc_gridsearch.best_params_\n",
    "rfc_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.70574865538574005"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier Model\n",
    "gbc_grid = {'learning_rate': [0.01,0.1],\n",
    "            'n_estimators': [100, 300],\n",
    "            'max_features': ['sqrt']}\n",
    "gbc_gridsearch = GridSearchCV(GradientBoostingClassifier(),\n",
    "                                 gbc_grid,\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=True,)\n",
    "gbc_gridsearch.fit(df_train_X, df_train_y)\n",
    "\n",
    "gbc_gridsearch.best_params_\n",
    "gbc_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test \n",
    "get_unigrams(df_test)\n",
    "get_common_unigram_ratio(df_test)\n",
    "get_bigrams(df_test)\n",
    "get_common_bigram_ratio(df_test)\n",
    "get_trigrams(df_test)\n",
    "get_common_trigram_ratio(df_test)\n",
    "get_cosine_similarity(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.71\n",
      "Recall Score= 0.62\n",
      "Precision Score = 0.60\n",
      "AUC_Score = 0.79\n"
     ]
    }
   ],
   "source": [
    "df_test_X = df_test[feature_cols]\n",
    "df_test_y = df_test.is_duplicate\n",
    "\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.1, max_features='sqrt', n_estimators=300)\n",
    "gbc.fit(df_train_X, df_train_y)\n",
    "\n",
    "def get_scores(model, df_test_X, df_test_y):\n",
    "    model_accuracy_score = accuracy_score(df_test_y, model.predict(df_test_X))\n",
    "    model_recall_score = recall_score(df_test_y, model.predict(df_test_X))\n",
    "    model_precision_score = precision_score(df_test_y, model.predict(df_test_X))\n",
    "    model_roc_auc_score = roc_auc_score(df_test_y, model.predict_proba(df_test_X)[:,1])\n",
    "    print 'Accuracy Score = {:.2f}\\nRecall Score= {:.2f}\\nPrecision Score = {:.2f}\\nAUC_Score = {:.2f}'.\\\n",
    "      format(model_accuracy_score, model_recall_score, model_precision_score, model_roc_auc_score)\n",
    "        \n",
    "get_scores(gbc, df_test_X, df_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Word2Vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure the logging model so that Word2Vec creates nice output messages.\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 56111 words, keeping 12256 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 112453 words, keeping 17931 word types\n",
      "INFO : PROGRESS: at sentence #30000, processed 168482 words, keeping 22258 word types\n",
      "INFO : PROGRESS: at sentence #40000, processed 224152 words, keeping 25857 word types\n",
      "INFO : PROGRESS: at sentence #50000, processed 280423 words, keeping 29198 word types\n",
      "INFO : PROGRESS: at sentence #60000, processed 336549 words, keeping 32150 word types\n",
      "INFO : PROGRESS: at sentence #70000, processed 392599 words, keeping 34782 word types\n",
      "INFO : PROGRESS: at sentence #80000, processed 448754 words, keeping 37080 word types\n",
      "INFO : PROGRESS: at sentence #90000, processed 504837 words, keeping 39319 word types\n",
      "INFO : PROGRESS: at sentence #100000, processed 560920 words, keeping 41487 word types\n",
      "INFO : PROGRESS: at sentence #110000, processed 616309 words, keeping 43512 word types\n",
      "INFO : PROGRESS: at sentence #120000, processed 672270 words, keeping 45437 word types\n",
      "INFO : PROGRESS: at sentence #130000, processed 728479 words, keeping 47385 word types\n",
      "INFO : PROGRESS: at sentence #140000, processed 784614 words, keeping 49130 word types\n",
      "INFO : PROGRESS: at sentence #150000, processed 841026 words, keeping 50933 word types\n",
      "INFO : PROGRESS: at sentence #160000, processed 897564 words, keeping 52668 word types\n",
      "INFO : PROGRESS: at sentence #170000, processed 953814 words, keeping 54306 word types\n",
      "INFO : PROGRESS: at sentence #180000, processed 1010308 words, keeping 55952 word types\n",
      "INFO : PROGRESS: at sentence #190000, processed 1066547 words, keeping 57490 word types\n",
      "INFO : PROGRESS: at sentence #200000, processed 1122200 words, keeping 58998 word types\n",
      "INFO : PROGRESS: at sentence #210000, processed 1177877 words, keeping 60438 word types\n",
      "INFO : PROGRESS: at sentence #220000, processed 1233798 words, keeping 61786 word types\n",
      "INFO : PROGRESS: at sentence #230000, processed 1290512 words, keeping 63198 word types\n",
      "INFO : PROGRESS: at sentence #240000, processed 1346478 words, keeping 64511 word types\n",
      "INFO : PROGRESS: at sentence #250000, processed 1403207 words, keeping 65844 word types\n",
      "INFO : PROGRESS: at sentence #260000, processed 1459816 words, keeping 67199 word types\n",
      "INFO : PROGRESS: at sentence #270000, processed 1516414 words, keeping 68466 word types\n",
      "INFO : PROGRESS: at sentence #280000, processed 1572474 words, keeping 69715 word types\n",
      "INFO : PROGRESS: at sentence #290000, processed 1628726 words, keeping 70947 word types\n",
      "INFO : PROGRESS: at sentence #300000, processed 1684448 words, keeping 72190 word types\n",
      "INFO : PROGRESS: at sentence #310000, processed 1740623 words, keeping 73373 word types\n",
      "INFO : PROGRESS: at sentence #320000, processed 1796623 words, keeping 74533 word types\n",
      "INFO : PROGRESS: at sentence #330000, processed 1853115 words, keeping 75728 word types\n",
      "INFO : PROGRESS: at sentence #340000, processed 1909090 words, keeping 76798 word types\n",
      "INFO : PROGRESS: at sentence #350000, processed 1965343 words, keeping 77836 word types\n",
      "INFO : PROGRESS: at sentence #360000, processed 2021416 words, keeping 78887 word types\n",
      "INFO : PROGRESS: at sentence #370000, processed 2077700 words, keeping 79936 word types\n",
      "INFO : PROGRESS: at sentence #380000, processed 2134143 words, keeping 81020 word types\n",
      "INFO : PROGRESS: at sentence #390000, processed 2191169 words, keeping 82129 word types\n",
      "INFO : PROGRESS: at sentence #400000, processed 2247883 words, keeping 83139 word types\n",
      "INFO : PROGRESS: at sentence #410000, processed 2304678 words, keeping 84102 word types\n",
      "INFO : PROGRESS: at sentence #420000, processed 2361195 words, keeping 84998 word types\n",
      "INFO : PROGRESS: at sentence #430000, processed 2418152 words, keeping 85927 word types\n",
      "INFO : PROGRESS: at sentence #440000, processed 2474678 words, keeping 86737 word types\n",
      "INFO : PROGRESS: at sentence #450000, processed 2531728 words, keeping 87538 word types\n",
      "INFO : PROGRESS: at sentence #460000, processed 2588525 words, keeping 88352 word types\n",
      "INFO : PROGRESS: at sentence #470000, processed 2645234 words, keeping 89172 word types\n",
      "INFO : PROGRESS: at sentence #480000, processed 2702050 words, keeping 89998 word types\n",
      "INFO : PROGRESS: at sentence #490000, processed 2758975 words, keeping 90797 word types\n",
      "INFO : PROGRESS: at sentence #500000, processed 2815981 words, keeping 91620 word types\n",
      "INFO : PROGRESS: at sentence #510000, processed 2871783 words, keeping 92401 word types\n",
      "INFO : PROGRESS: at sentence #520000, processed 2928366 words, keeping 93162 word types\n",
      "INFO : PROGRESS: at sentence #530000, processed 2985519 words, keeping 93887 word types\n",
      "INFO : PROGRESS: at sentence #540000, processed 3042678 words, keeping 94700 word types\n",
      "INFO : PROGRESS: at sentence #550000, processed 3099976 words, keeping 95493 word types\n",
      "INFO : PROGRESS: at sentence #560000, processed 3157021 words, keeping 96247 word types\n",
      "INFO : PROGRESS: at sentence #570000, processed 3214141 words, keeping 96968 word types\n",
      "INFO : PROGRESS: at sentence #580000, processed 3271251 words, keeping 97687 word types\n",
      "INFO : PROGRESS: at sentence #590000, processed 3328659 words, keeping 98396 word types\n",
      "INFO : PROGRESS: at sentence #600000, processed 3384822 words, keeping 99067 word types\n",
      "INFO : PROGRESS: at sentence #610000, processed 3441830 words, keeping 99820 word types\n",
      "INFO : PROGRESS: at sentence #620000, processed 3498633 words, keeping 100541 word types\n",
      "INFO : PROGRESS: at sentence #630000, processed 3555337 words, keeping 101294 word types\n",
      "INFO : PROGRESS: at sentence #640000, processed 3611415 words, keeping 101960 word types\n",
      "INFO : PROGRESS: at sentence #650000, processed 3668416 words, keeping 102663 word types\n",
      "INFO : PROGRESS: at sentence #660000, processed 3725045 words, keeping 103401 word types\n",
      "INFO : PROGRESS: at sentence #670000, processed 3782225 words, keeping 104121 word types\n",
      "INFO : PROGRESS: at sentence #680000, processed 3839545 words, keeping 104864 word types\n",
      "INFO : PROGRESS: at sentence #690000, processed 3896364 words, keeping 105554 word types\n",
      "INFO : PROGRESS: at sentence #700000, processed 3953033 words, keeping 106268 word types\n",
      "INFO : PROGRESS: at sentence #710000, processed 4009315 words, keeping 106939 word types\n",
      "INFO : PROGRESS: at sentence #720000, processed 4066213 words, keeping 107628 word types\n",
      "INFO : PROGRESS: at sentence #730000, processed 4122484 words, keeping 108278 word types\n",
      "INFO : PROGRESS: at sentence #740000, processed 4179268 words, keeping 108921 word types\n",
      "INFO : PROGRESS: at sentence #750000, processed 4235830 words, keeping 109577 word types\n",
      "INFO : PROGRESS: at sentence #760000, processed 4292783 words, keeping 110223 word types\n",
      "INFO : PROGRESS: at sentence #770000, processed 4349687 words, keeping 110901 word types\n",
      "INFO : PROGRESS: at sentence #780000, processed 4406441 words, keeping 111532 word types\n",
      "INFO : PROGRESS: at sentence #790000, processed 4463391 words, keeping 112226 word types\n",
      "INFO : PROGRESS: at sentence #800000, processed 4521057 words, keeping 112846 word types\n",
      "INFO : collected 113393 word types from a corpus of 4570453 raw words and 808576 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=1 retains 113393 unique words (100% of original 113393, drops 0)\n",
      "INFO : min_count=1 leaves 4570453 word corpus (100% of original 4570453, drops 0)\n",
      "INFO : deleting the raw counts dictionary of 113393 items\n",
      "INFO : sample=0.001 downsamples 25 most-common words\n",
      "INFO : downsampling leaves estimated 4381632 word corpus (95.9% of prior 4570453)\n",
      "INFO : estimated required memory for 113393 words and 300 dimensions: 328839700 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 113393 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : PROGRESS: at 3.13% examples, 675762 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : PROGRESS: at 6.43% examples, 695232 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 9.81% examples, 709464 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 13.17% examples, 716084 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 16.60% examples, 721182 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: at 20.12% examples, 728482 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 23.61% examples, 728968 words/s, in_qsize 8, out_qsize 1\n",
      "INFO : PROGRESS: at 27.08% examples, 732129 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 30.46% examples, 731481 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 33.81% examples, 731364 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 37.24% examples, 732984 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 40.64% examples, 734099 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 44.08% examples, 734506 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 47.60% examples, 736220 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 50.97% examples, 735883 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 54.28% examples, 735275 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 57.67% examples, 735768 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 60.94% examples, 734725 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 64.25% examples, 733320 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 67.64% examples, 733137 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 70.96% examples, 732173 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 74.40% examples, 732874 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 77.75% examples, 732748 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 81.11% examples, 732808 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : PROGRESS: at 84.46% examples, 732560 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 87.80% examples, 732068 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 90.74% examples, 728633 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 94.08% examples, 728887 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 97.48% examples, 729410 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : training on 22852265 raw words (21908375 effective words) took 30.0s, 730415 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.77\n",
      "Recall Score= 0.57\n",
      "Precision Score = 0.75\n",
      "AUC_Score = 0.82\n"
     ]
    }
   ],
   "source": [
    "# Get unigrams of the whole dataset.\n",
    "get_unigrams(df)\n",
    "# Combine question1 and question2 to list.\n",
    "sentences = df.question1_unigram.tolist() + df.question2_unigram.values.tolist()\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model.\n",
    "model_self_trained = Word2Vec(sentences, workers=num_workers, \\\n",
    "                 size=num_features, min_count=min_word_count, \\\n",
    "                 window=context, sample=downsampling)\n",
    "# Get the names of the words in the model's vocabulary. Convert it to a set, for speed.\n",
    "model_words = set()\n",
    "for item in sentences:\n",
    "    for i in item:\n",
    "        model_words.add(i)\n",
    "    \n",
    "# Function to average all of the word vectors.\n",
    "def make_feature_vec(df, model, model_words, num_features):\n",
    "    counter = 0\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    df_vecs = np.zeros((len(df), num_features), dtype= 'float32')\n",
    "    for i in df.index.values:\n",
    "        words = df.question1_unigram[i] + df.question2_unigram[i]\n",
    "        set_words = set(words)\n",
    "        nwords = 0.\n",
    "        feature_vecs = np.zeros((num_features,), dtype= 'float32')\n",
    "    #  Loop over each word in the question1 and question2 and, if it is in the model's vocaublary,\\\n",
    "    #  add its feature vector to the total.\n",
    "        for word in set_words:\n",
    "            if word in model_words: \n",
    "                nwords = nwords + 1\n",
    "                feature_vecs = np.add(feature_vecs,model[word])\n",
    "        df_vecs[counter] = np.divide(feature_vecs, nwords + 1.)                                 \n",
    "        counter += 1\n",
    "    return df_vecs\n",
    "\n",
    "# Get training dataset vectors.\n",
    "df_train_vecs_s = make_feature_vec(df_train, model_self_trained, model_words, num_features)\n",
    "# Get testing dataset vectors.\n",
    "df_test_vecs_s = make_feature_vec(df_test, model_self_trained, model_words, num_features)\n",
    "\n",
    "# Fit the model\n",
    "rmc = RandomForestClassifier(n_estimators=10)\n",
    "rmc.fit(df_train_vecs_s, df_train_y)\n",
    "get_scores(rmc, df_test_vecs_s, df_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808576"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def default_clean(text):\n",
    "    '''\n",
    "    Removes default bad characters\n",
    "    '''\n",
    "    if not (pd.isnull(text)):\n",
    "        text = filter(lambda x: x in string.printable, text)\n",
    "        bad_chars = set([\"@\", \"+\", '<br>', '<br />', '/', \"'\", '\"', '\\\\',\n",
    "                        '(',')', '<p>', '\\\\n', '<', '>', '?', '#', ',',\n",
    "                        '.', '[',']', '%', '$', '&', ';', '!', ';', ':',\n",
    "                        '-', \"*\", \"_\", \"=\", \"}\", \"{\"])\n",
    "        for char in bad_chars:\n",
    "            text = text.replace(char, \" \")\n",
    "        text = re.sub('\\d+', \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def stop_and_stem(text, stem=True, stemmer = PorterStemmer()):\n",
    "    '''\n",
    "    Removes stopwords and does stemming\n",
    "    '''\n",
    "    stoplist = stopwords.words('english')\n",
    "\n",
    "    if stem:\n",
    "        text_stemmed = [[stemmer.stem(word) for word in document.lower().split()\n",
    "                         if word not in stoplist] for document in text]\n",
    "    else:\n",
    "        text_stemmed = [[word for word in document.lower().split()\n",
    "                 if word not in stoplist] for document in text]\n",
    "\n",
    "    return text_stemmed\n",
    "\n",
    "\n",
    "def make_corpus(parsed_text, bigrams=True, filter_extremes=False, below=5, above=0.1):\n",
    "    '''\n",
    "    Prepares corpus and dictionary with options for removing outliers or using bigrams\n",
    "    '''\n",
    "\n",
    "    if bigrams:\n",
    "        bigrams = Phrases(parsed_text)\n",
    "        corpora_dict = corpora.Dictionary(bigrams[parsed_text])\n",
    "        parsed_text = bigrams[parsed_text]\n",
    "    else:\n",
    "        corpora_dict = corpora.Dictionary(parsed_text)\n",
    "\n",
    "    # Filter the dict to remove redundant words\n",
    "    if filter_extremes:\n",
    "        print(\"Size of dict before filter: %s\") % len(corpora_dict)\n",
    "        corpora_dict.filter_extremes(no_below=below, no_above=above)\n",
    "        print(\"Size of dict after filter: %s\") % len(corpora_dict)\n",
    "\n",
    "\n",
    "    # Convert the cleaned documents into bag of words\n",
    "    corpus = [corpora_dict.doc2bow(t) for t in parsed_text]\n",
    "\n",
    "    return corpora_dict, corpus\n",
    "\n",
    "\n",
    "def data_transformation(input_data, bigrams=True, stem_flag=True, filter_extremes=False, below=5, above=0.1):\n",
    "    '''\n",
    "    Combines all data transformation steps: clean, drop stopwords, stem, make corpus\n",
    "    '''\n",
    "\n",
    "    clean_reviews = [default_clean(d).lower() for d in input_data]\n",
    "    stemmed = stop_and_stem(clean_reviews, stem=stem_flag)\n",
    "    dictn, data = make_corpus(stemmed, bigrams=bigrams, filter_extremes=filter_extremes,\n",
    "                              below=below, above=above)\n",
    "\n",
    "    return data, dictn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(10 unique tokens: [u'tripadvisor', u'nsurlnamekey', u'data', u'nsurlfilesizekey', u'bplist']...) from 2 documents (total 10 corpus positions)\n",
      "INFO : discarding 10 tokens: [(u'tripadvisor', 1), (u'nsurlnamekey', 1), (u'xls', 1), (u'nsurlfilesizekey', 1), (u'data', 1), (u'v', 1), (u'nsurlfileresourcetypekey', 1), (u'bplist', 1), (u'jdi', 1), (u'nsurlfileresourcetyperegular', 1)]...\n",
      "INFO : keeping 0 tokens which were in no less than 5 and no more than 0 (=20.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dict before filter: 10\n",
      "Size of dict after filter: 0\n"
     ]
    }
   ],
   "source": [
    "# clean up raw reviews and prepare dataset for model\n",
    "corpus, dictionary = data_transformation(reviews, bigrams=False,\\\n",
    "                                         stem_flag=False, filter_extremes=True, below=5, above=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x82\\x00_\\x10\\x1cNSURLFileResourceTypeRegular\\x08\\x0f\\x1c/Jdi\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x88'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews=[] \n",
    "# directory = os.path.normpath(\"data\")\n",
    "# for subdir, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".txt\"):\n",
    "#             f=open(os.path.join(subdir, file),'r')\n",
    "#             raw=f.read()\n",
    "#             f.close()\n",
    "#             lines = raw.splitlines() # split on lines and carriages \\n\\r\n",
    "           \n",
    "#             print len(lines[1].strip('<Content>').split('\\n')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jNlp.jTokenize import jTokenize\n",
    "input_sentence = u'私は彼を５日前、つまりこの前の金曜日に駅で見かけた'\n",
    "list_of_tokens = jTokenize(input_sentence)\n",
    "print '--'.join(list_of_tokens).encode('utf-8')\n",
    "from jNlp.jCabocha import cabocha\n",
    "print cabocha(input_sentence).encode('utf-8')\n",
    "\n",
    "from jNlp.jConvert import *\n",
    "input_sentence = u'気象庁が２１日午前４時４８分、発表した天気概況によると、'\n",
    "print ' '.join(tokenizedRomaji(input_sentence))\n",
    "print tokenizedRomaji(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jNlp.jProcessing import long_substr\n",
    "a = u'これでアナタも冷え知らず'\n",
    "b = u'これでア冷え知らずナタも'\n",
    "print long_substr(a, b).encode('utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jNlp.jProcessing import Similarities\n",
    "s = Similarities()\n",
    "a = u'これは何ですか？'\n",
    "b = u'これはわからないです'\n",
    "print s.minhash(' '.join(jTokenize(a)), ' '.join(jTokenize(b)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jNlp.jSentiments import *\n",
    "classifier = Sentiment()\n",
    "classifier.train(en_swn, jp_wn)\n",
    "text = u'監督、俳優、ストーリー、演出、全部最高！'\n",
    "print classifier.baseline(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/koyuki.nakamori/anaconda/lib/python2.7/site-packages\n",
      "\u001b[31mtensorflow-1.0.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
      "Requirement already satisfied: bokeh in /Users/koyuki.nakamori/anaconda/lib/python2.7/site-packages\n"
     ]
    }
   ],
   "source": [
    "##pip install --upgrade gensim\n",
    "## pip install nltk\n",
    "!pip install tqdm\n",
    "!pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\n",
    "#pip install keras\n",
    "!pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(32 unique tokens: [u'often', u'feel', u'profession', u'drive', u'say']...) from 5 documents (total 46 corpus positions)\n",
      "INFO : using symmetric alpha at 0.5\n",
      "INFO : using symmetric eta at 0.03125\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 2 topics, 20 passes over the supplied corpus of 5 documents, updating model once every 5 documents, evaluating perplexity every 5 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : -4.352 per-word bound, 20.4 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 0, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.049*\"pressur\" + 0.045*\"drive\" + 0.045*\"health\" + 0.041*\"mother\" + 0.037*\"brother\" + 0.037*\"may\" + 0.034*\"good\" + 0.034*\"tension\" + 0.034*\"increas\" + 0.034*\"caus\"\n",
      "INFO : topic #1 (0.500): 0.064*\"brocolli\" + 0.060*\"good\" + 0.059*\"brother\" + 0.057*\"mother\" + 0.055*\"health\" + 0.055*\"drive\" + 0.045*\"eat\" + 0.032*\"pressur\" + 0.028*\"like\" + 0.027*\"time\"\n",
      "INFO : topic diff=0.347534, rho=1.000000\n",
      "INFO : -4.087 per-word bound, 17.0 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 1, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.054*\"pressur\" + 0.051*\"drive\" + 0.051*\"health\" + 0.048*\"may\" + 0.046*\"tension\" + 0.046*\"increas\" + 0.046*\"caus\" + 0.045*\"expert\" + 0.045*\"blood\" + 0.044*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.066*\"brocolli\" + 0.065*\"good\" + 0.064*\"brother\" + 0.063*\"mother\" + 0.051*\"health\" + 0.051*\"drive\" + 0.047*\"eat\" + 0.030*\"pressur\" + 0.029*\"like\" + 0.028*\"time\"\n",
      "INFO : topic diff=0.292352, rho=0.577350\n",
      "INFO : -3.969 per-word bound, 15.7 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 2, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.056*\"pressur\" + 0.055*\"health\" + 0.054*\"drive\" + 0.052*\"may\" + 0.051*\"tension\" + 0.051*\"increas\" + 0.051*\"caus\" + 0.051*\"expert\" + 0.051*\"blood\" + 0.050*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.066*\"good\" + 0.066*\"brother\" + 0.065*\"mother\" + 0.050*\"drive\" + 0.050*\"health\" + 0.048*\"eat\" + 0.029*\"pressur\" + 0.029*\"like\" + 0.028*\"time\"\n",
      "INFO : topic diff=0.152068, rho=0.500000\n",
      "INFO : -3.938 per-word bound, 15.3 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 3, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.057*\"pressur\" + 0.056*\"health\" + 0.056*\"drive\" + 0.054*\"may\" + 0.054*\"tension\" + 0.054*\"increas\" + 0.054*\"caus\" + 0.054*\"expert\" + 0.053*\"blood\" + 0.053*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"good\" + 0.066*\"brother\" + 0.066*\"mother\" + 0.049*\"drive\" + 0.049*\"health\" + 0.048*\"eat\" + 0.029*\"pressur\" + 0.029*\"like\" + 0.029*\"time\"\n",
      "INFO : topic diff=0.080313, rho=0.447214\n",
      "INFO : -3.929 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 4, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.058*\"health\" + 0.058*\"pressur\" + 0.057*\"drive\" + 0.055*\"may\" + 0.055*\"tension\" + 0.055*\"increas\" + 0.055*\"caus\" + 0.055*\"expert\" + 0.055*\"blood\" + 0.055*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"good\" + 0.067*\"brother\" + 0.067*\"mother\" + 0.049*\"drive\" + 0.048*\"health\" + 0.048*\"eat\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.044800, rho=0.408248\n",
      "INFO : -3.926 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 5, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.058*\"health\" + 0.058*\"pressur\" + 0.057*\"drive\" + 0.056*\"may\" + 0.056*\"tension\" + 0.056*\"increas\" + 0.056*\"caus\" + 0.056*\"expert\" + 0.056*\"blood\" + 0.056*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"good\" + 0.067*\"brother\" + 0.067*\"mother\" + 0.048*\"drive\" + 0.048*\"eat\" + 0.048*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.026159, rho=0.377964\n",
      "INFO : -3.925 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 6, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.057*\"drive\" + 0.056*\"may\" + 0.056*\"tension\" + 0.056*\"increas\" + 0.056*\"caus\" + 0.056*\"expert\" + 0.056*\"blood\" + 0.056*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"good\" + 0.067*\"brother\" + 0.067*\"mother\" + 0.048*\"drive\" + 0.048*\"eat\" + 0.048*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.015891, rho=0.353553\n",
      "INFO : -3.925 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 7, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.056*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"good\" + 0.067*\"brother\" + 0.067*\"mother\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.009991, rho=0.333333\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 8, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"brother\" + 0.067*\"good\" + 0.067*\"mother\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.006470, rho=0.316228\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 9, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"brother\" + 0.067*\"good\" + 0.067*\"mother\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.004300, rho=0.301511\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 10, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"brother\" + 0.067*\"good\" + 0.067*\"mother\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.002923, rho=0.288675\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 11, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brocolli\" + 0.067*\"brother\" + 0.067*\"mother\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"time\" + 0.029*\"basebal\"\n",
      "INFO : topic diff=0.002025, rho=0.277350\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 12, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.059*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brother\" + 0.067*\"brocolli\" + 0.067*\"mother\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"time\"\n",
      "INFO : topic diff=0.001428, rho=0.267261\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 13, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.500): 0.067*\"brother\" + 0.067*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.001022, rho=0.258199\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 14, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brother\" + 0.067*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.000742, rho=0.250000\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 15, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.067*\"brother\" + 0.067*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.000545, rho=0.242536\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 16, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.068*\"brother\" + 0.067*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.000406, rho=0.235702\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 17, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.068*\"brother\" + 0.068*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.000304, rho=0.229416\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 18, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.068*\"brother\" + 0.068*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.000230, rho=0.223607\n",
      "INFO : -3.924 per-word bound, 15.2 perplexity estimate based on a held-out corpus of 5 documents with 46 words\n",
      "INFO : PROGRESS: pass 19, at document #5/5\n",
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\" + 0.057*\"tension\" + 0.057*\"increas\" + 0.057*\"caus\" + 0.057*\"expert\" + 0.057*\"blood\" + 0.057*\"suggest\"\n",
      "INFO : topic #1 (0.500): 0.068*\"brother\" + 0.068*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\" + 0.048*\"eat\" + 0.048*\"drive\" + 0.047*\"health\" + 0.029*\"like\" + 0.029*\"seem\" + 0.029*\"well\"\n",
      "INFO : topic diff=0.000176, rho=0.218218\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.500): 0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\"\n",
      "INFO : topic #1 (0.500): 0.068*\"brother\" + 0.068*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.060*\"health\" + 0.058*\"pressur\" + 0.058*\"drive\" + 0.057*\"may\"'), (1, u'0.068*\"brother\" + 0.068*\"mother\" + 0.067*\"brocolli\" + 0.067*\"good\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
